{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AU3aenm0Zota",
        "DzQs9WJngfsp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDEsdD3Wsda",
        "colab_type": "text"
      },
      "source": [
        "#### **Named Entity Recognition using LSTM**\n",
        "\n",
        "The NLP task [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) is to classify named entity's within a corpus into predefined categories.\n",
        "\n",
        "We explore the use of a [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) architecture to perform NER. The dataset used is the MIT-Restaurants dataset. The tagging of the dataset is in the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.\n",
        "\n",
        "Note: Framework based off https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2.ipynb\n",
        "\n",
        "Note: For a gentler introduction to the LSTM architecture see [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GgGlp7JYtiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "8b3d0a50-1e93-4262-8984-99ee938f1086"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import util\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\" # use a gpu!"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fc23da811d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3aenm0Zota",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Read in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRcN9c6leOWo",
        "colab_type": "code",
        "outputId": "50380f20-ff05-4f58-b6d3-1ea2239dd45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data = util.read_file_txt(\"../data/mit_restaurants-train.dat\", type = \"word\")\n",
        "train_tags = util.read_file_txt(\"../data/mit_restaurants-train.tag\", type = \"word\")\n",
        "\n",
        "test_data = util.read_file_txt(\"../data/mit_restaurants-test.dat\", type = \"word\")\n",
        "test_tags = util.read_file_txt(\"../data/mit_restaurants-test.tag\", type = \"word\")\n",
        "\n",
        "print('number of training samples:', len(train_data))\n",
        "print('number of testing samples:',  len(test_data))\n",
        "# print('average sentence length in training data', (np.mean([len(sent) for sent in train_data])))\n",
        "print()\n",
        "\n",
        "print('the first few sentences are:', train_data[0:3])\n",
        "print('and their cp\\'ding named entity sequences are: ', str(train_tags[0:3]))\n",
        "print()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples: 7660\n",
            "number of testing samples: 1521\n",
            "\n",
            "the first few sentences are: [['2', 'start', 'restaurants', 'with', 'inside', 'dining'], ['34'], ['5', 'star', 'resturants', 'in', 'my', 'town']]\n",
            "and their cp'ding named entity sequences are:  [['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity'], ['O'], ['B-Rating', 'I-Rating', 'O', 'B-Location', 'I-Location', 'I-Location']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzQs9WJngfsp",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44uSZVEXg48r",
        "colab_type": "code",
        "outputId": "2790fa27-3056-476d-a3df-6ee720ba3597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# helper functions and more data preprocessing before we move on to implementing our models.\n",
        "\n",
        "# from train data, collect all unique word types as a set and add 'UNK' to it (unseen words in test data will be turned into 'UNK')\n",
        "vocab_set = list(set([word for sent in train_data for word in sent])) + ['UNK']\n",
        "num_vocabs = len(vocab_set)\n",
        "print(\"number of word types (including 'UNK'):\", num_vocabs)\n",
        "print(\"the first couple and last couple of words in the vocabulary set:\", vocab_set[0:2] +  vocab_set[-2:])\n",
        "\n",
        "vocab2id = {v : i for i, v in enumerate(vocab_set)}\n",
        "\n",
        "#  collect all tag (class) types and assign an unique id to each of them. (here there won't be a unseen tag type in test data)\n",
        "tag_set = list(set([tag for tag_seq in train_tags for tag in tag_seq]))\n",
        "num_tags = len(tag_set)\n",
        "print(\"number of tag types:\", num_tags)\n",
        "print()\n",
        "\n",
        "# assign each tag type a unique id, also create the inverse dict of tag2id (required during evaluation)\n",
        "tag2id = {t : i for i, t in enumerate(tag_set)} \n",
        "id2tag = {i : t for t, i in tag2id.items()}\n",
        "\n",
        "# apply one-hot encoding to data.\n",
        "train_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in train_data]\n",
        "# print(\"oh data[0] - len:\", len(train_data_oh_list[0]), \"shape:\", train_data_oh_list[0].shape)\n",
        "\n",
        "# transform tag names into ids\n",
        "train_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in train_tags]\n",
        "# print(\"list len, tag data:\", len(train_tags_id_list))\n",
        "\n",
        "# train_data_oh_list should now be a list of 2d-tensors, each has shape (sent_len, num_vocabs)\n",
        "# Note that to utilize the `shape` attribute, each element in the list should already be a torch tensor.\n",
        "print(\"first sentence has shape: %s\" % str(train_data_oh_list[0].shape))\n",
        "print(\"fifth sentence has shape: %s\" % str(train_data_oh_list[4].shape))\n",
        "\n",
        "# train_tags_id_list is a list of 1d-tensors, each that has shape (sent_len,)\n",
        "print(\"first tag sequence has shape: %s\" % train_tags_id_list[0].shape)\n",
        "print(\"fifth tag sequence has shape: %s\" % train_tags_id_list[4].shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# Apply same conversion to test dataset.\n",
        "test_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in test_data]\n",
        "test_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in test_tags]\n",
        "# print(\"list len, oh test:\", len(test_data_oh_list))\n",
        "# print(\"list len, tag test:\", len(test_tags_id_list))\n",
        "# print(\"first sentence has shape: %s\" % str(test_data_oh_list[0].shape))\n",
        "# print(\"fifth sentence has shape: %s\" % str(test_data_oh_list[4].shape))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of word types (including 'UNK'): 3805\n",
            "the first couple and last couple of words in the vocabulary set: ['czech', 'sashimi', 'germantown', 'UNK']\n",
            "number of tag types: 17\n",
            "\n",
            "first sentence has shape: torch.Size([6, 3805])\n",
            "fifth sentence has shape: torch.Size([12, 3805])\n",
            "first tag sequence has shape: 6\n",
            "fifth tag sequence has shape: 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gCDSGGdfeZ",
        "colab_type": "text"
      },
      "source": [
        "#### **LSTM**\n",
        "\n",
        "We implement a vanilla LSTM from scratch, then train it and evaluate its performance on the NER task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37BjwIIef15",
        "colab_type": "text"
      },
      "source": [
        "##### **LSTM architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_t2jPVDd3bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.sz_ip = input_size\n",
        "    self.sz_hi = hidden_size\n",
        "    self.sz_op = output_size        \n",
        "\n",
        "    self.gate_ip = nn.Linear(self.sz_ip + self.sz_hi, self.sz_hi)   # input gate\n",
        "    self.gate_fg = nn.Linear(self.sz_ip + self.sz_hi, self.sz_hi)   # forget gate\n",
        "    self.gate_op = nn.Linear(self.sz_ip + self.sz_hi, self.sz_hi)   # output gate    \n",
        "    self.cell_cand = nn.Linear(self.sz_ip + self.sz_hi, self.sz_hi) # candidate cell\n",
        "            \n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "    # self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden, memory):\n",
        "    \"\"\"\n",
        "    The `forward` function just performs one step of update and output logits before softmax.\n",
        "\n",
        "    `input` is a 2d-tensor of shape (1, input_size);\n",
        "    `hidden` and `memory` are both 2d-tensors of shape (1, hidden_size), representing the hidden and memory states of the previous time step.\n",
        "    \"\"\"\n",
        "\n",
        "    combined = torch.cat( (input, hidden), dim = 1)\n",
        "    # print(\"input, combined, memory:\", input.shape, combined.shape, memory.shape)\n",
        "    \n",
        "    # tmp_1a = self.gate_fg(combined)\n",
        "    # tmp_1 = self.sigmoid(tmp_1a)\n",
        "    # tmp_2 = self.sigmoid(self.gate_ip(combined))\n",
        "    # print(\"tmp_1, tmp_2:\", tmp_1.shape, tmp_2.shape)\n",
        "    \n",
        "    # tmp_3 = self.sigmoid(self.gate_ip(combined))\n",
        "    # tmp_4 = self.tanh(self.cell_cand(combined))\n",
        "    # print(\"tmp_3, tmp_4:\", tmp_3.shape, tmp_4.shape)\n",
        "    \n",
        "    memory = (torch.sigmoid(self.gate_fg(combined)) * memory) + (torch.sigmoid(self.gate_ip(combined)) * torch.tanh(self.cell_cand(combined)))\n",
        "    # print(\"memory:\", memory.shape)\n",
        "    \n",
        "    hidden = torch.sigmoid(self.gate_op(combined)) * torch.tanh(memory)\n",
        "    # print(\"output:\", output.shape)\n",
        "    \n",
        "    output = hidden\n",
        "\n",
        "    return output, hidden, memory\n",
        "\n",
        "  def init_state(self):\n",
        "    \"\"\"     # initialize hidden and memory states.\"\"\"\n",
        "    return (torch.zeros(1, self.sz_hi).to(device), torch.zeros(1, self.sz_hi).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQfXsM0dz4m",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**\n",
        "\n",
        "The function $train\\_one\\_sample$, takes a (sentence tensor, tag tensor) pair as input and does one step of the gradient update.\n",
        "\n",
        "The input size is set to the vocbulary size of the dataset, the size of the hidden state is set to $128$ and the output size is set to the number of tags in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrT97TNWkHF",
        "colab_type": "code",
        "outputId": "8799d20d-e228-4a47-b4a7-afa4d5e0ffe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_rate = 1e-3\n",
        "hidden_size = 128\n",
        "\n",
        "\n",
        "model = LSTM(input_size = num_vocabs, hidden_size = hidden_size, output_size = num_tags).to(device)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Run through a sentence, generate output, compute loss, and perform one gradient update\n",
        "Sentence and tag are represented as a 2d-tensor `sent_tensor` and a 1d-tensor `tag_tensor`, respectively.\n",
        "\"\"\"\n",
        "def train_one_sample(model, sent_tensor, tag_tensor):\n",
        "  hidden, memory = model.init_state()     # initialize hidden state\n",
        "  loss = 0.00\n",
        "\n",
        "  outputs = torch.zeros( (1, num_tags)).to(device)\n",
        "\n",
        "  # print(sent_tensor.device, tag_tensor.device, hidden.device, outputs.device, (torch.cuda.LongTensor([tag_tensor[0]]).to(device)).device)\n",
        "\n",
        "  for idx in range(sent_tensor.shape[0]):\n",
        "    # print(model.hidden.weight.grad, model.output.weight.grad)\n",
        "    outputs, hidden, memory = model(sent_tensor[idx].reshape(1, sent_tensor.shape[1]), hidden, memory)\n",
        "    loss = loss + criterion(outputs, torch.LongTensor([tag_tensor[idx]]).to(device))\n",
        "      \n",
        "  loss = loss / len(tag_tensor)   # average the loss over all tags in the sentance\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return outputs, loss.item()\n",
        "\n",
        "\n",
        "# main training loop for the lstm\n",
        "num_epochs = 10\n",
        "iter_count = 0\n",
        "print_every = 1000\n",
        "plot_every = 50\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "b_calc_grad_norm = True # set to true if we want to analyze how the gradient changes over training\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  current_grad_norm = 0\n",
        "  all_grad_norms = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model.train()\n",
        " \n",
        "for idx_epoch in range(num_epochs):\n",
        "  for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    tag_tensor = tag_tensor.to(device)\n",
        "\n",
        "    output, loss = train_one_sample(model, sent_tensor, tag_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    if b_calc_grad_norm:\n",
        "      grad = torch.FloatTensor( (1, 1)).to(device)\n",
        "      \n",
        "      for param in model.parameters():\n",
        "        grad = torch.cat( (grad, torch.reshape(param.grad, (-1, ))))\n",
        "      \n",
        "      current_grad_norm += torch.norm(grad.to(\"cpu\"), 2)\n",
        "\n",
        "    if iter_count % print_every == 0:\n",
        "      if b_calc_grad_norm:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}, grad_norm: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss, torch.norm(grad.to(\"cpu\"), 2)))\n",
        "      else:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss))\n",
        "\n",
        "\n",
        "    # add current loss avg to list of losses\n",
        "    if iter_count % plot_every == 0 and iter_count > 0:\n",
        "      all_losses.append(current_loss / plot_every)\n",
        "      current_loss = 0\n",
        "      \n",
        "      if b_calc_grad_norm: \n",
        "        all_grad_norms.append(current_grad_norm / plot_every)\n",
        "        current_grad_norm = 0\n",
        "\n",
        "    iter_count += 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, iteration: 0, time: 0m 0s, loss: 4.8556, grad_norm: 1.4337\n",
            "epoch: 0, iteration: 1000, time: 0m 30s, loss: 4.0498, grad_norm: 1.4524\n",
            "epoch: 0, iteration: 2000, time: 1m 0s, loss: 3.4869, grad_norm: 1.4209\n",
            "epoch: 0, iteration: 3000, time: 1m 30s, loss: 3.7235, grad_norm: 1.4345\n",
            "epoch: 0, iteration: 4000, time: 2m 1s, loss: 3.4475, grad_norm: 1.4373\n",
            "epoch: 0, iteration: 5000, time: 2m 32s, loss: 3.3240, grad_norm: 1.4209\n",
            "epoch: 0, iteration: 6000, time: 3m 1s, loss: 3.4700, grad_norm: 1.4185\n",
            "epoch: 0, iteration: 7000, time: 3m 30s, loss: 3.4049, grad_norm: 1.4392\n",
            "epoch: 1, iteration: 8000, time: 3m 59s, loss: 3.2616, grad_norm: 1.4264\n",
            "epoch: 1, iteration: 9000, time: 4m 30s, loss: 3.5734, grad_norm: 1.4424\n",
            "epoch: 1, iteration: 10000, time: 4m 59s, loss: 3.1938, grad_norm: 1.4202\n",
            "epoch: 1, iteration: 11000, time: 5m 30s, loss: 3.1242, grad_norm: 1.4266\n",
            "epoch: 1, iteration: 12000, time: 6m 0s, loss: 3.3821, grad_norm: 1.4169\n",
            "epoch: 1, iteration: 13000, time: 6m 30s, loss: 3.3084, grad_norm: 1.4612\n",
            "epoch: 1, iteration: 14000, time: 6m 59s, loss: 3.3244, grad_norm: 1.4227\n",
            "epoch: 1, iteration: 15000, time: 7m 28s, loss: 3.3686, grad_norm: 1.4338\n",
            "epoch: 2, iteration: 16000, time: 7m 58s, loss: 3.4087, grad_norm: 1.4159\n",
            "epoch: 2, iteration: 17000, time: 8m 28s, loss: 3.3549, grad_norm: 1.4652\n",
            "epoch: 2, iteration: 18000, time: 8m 57s, loss: 3.2181, grad_norm: 1.4291\n",
            "epoch: 2, iteration: 19000, time: 9m 29s, loss: 3.1875, grad_norm: 1.4163\n",
            "epoch: 2, iteration: 20000, time: 9m 59s, loss: 3.3194, grad_norm: 1.4633\n",
            "epoch: 2, iteration: 21000, time: 10m 28s, loss: 3.1780, grad_norm: 1.4173\n",
            "epoch: 2, iteration: 22000, time: 10m 58s, loss: 3.4340, grad_norm: 1.4204\n",
            "epoch: 3, iteration: 23000, time: 11m 26s, loss: 3.4988, grad_norm: 1.4168\n",
            "epoch: 3, iteration: 24000, time: 11m 56s, loss: 3.2710, grad_norm: 1.4167\n",
            "epoch: 3, iteration: 25000, time: 12m 25s, loss: 3.1145, grad_norm: 1.4187\n",
            "epoch: 3, iteration: 26000, time: 12m 55s, loss: 3.1948, grad_norm: 1.4345\n",
            "epoch: 3, iteration: 27000, time: 13m 26s, loss: 3.5290, grad_norm: 1.4159\n",
            "epoch: 3, iteration: 28000, time: 13m 56s, loss: 3.3109, grad_norm: 1.4159\n",
            "epoch: 3, iteration: 29000, time: 14m 26s, loss: 3.6773, grad_norm: 1.4471\n",
            "epoch: 3, iteration: 30000, time: 14m 55s, loss: 3.2647, grad_norm: 1.4318\n",
            "epoch: 4, iteration: 31000, time: 15m 24s, loss: 3.3601, grad_norm: 1.4208\n",
            "epoch: 4, iteration: 32000, time: 15m 55s, loss: 3.5136, grad_norm: 1.4160\n",
            "epoch: 4, iteration: 33000, time: 16m 24s, loss: 3.2217, grad_norm: 1.4417\n",
            "epoch: 4, iteration: 34000, time: 16m 55s, loss: 3.4213, grad_norm: 1.4375\n",
            "epoch: 4, iteration: 35000, time: 17m 26s, loss: 3.2022, grad_norm: 1.4195\n",
            "epoch: 4, iteration: 36000, time: 17m 56s, loss: 3.3997, grad_norm: 1.4268\n",
            "epoch: 4, iteration: 37000, time: 18m 25s, loss: 3.1494, grad_norm: 1.4150\n",
            "epoch: 4, iteration: 38000, time: 18m 54s, loss: 3.4390, grad_norm: 1.4234\n",
            "epoch: 5, iteration: 39000, time: 19m 23s, loss: 3.2454, grad_norm: 1.4270\n",
            "epoch: 5, iteration: 40000, time: 19m 53s, loss: 3.6064, grad_norm: 1.4337\n",
            "epoch: 5, iteration: 41000, time: 20m 22s, loss: 3.4108, grad_norm: 1.4156\n",
            "epoch: 5, iteration: 42000, time: 20m 54s, loss: 3.4868, grad_norm: 1.4594\n",
            "epoch: 5, iteration: 43000, time: 21m 23s, loss: 3.1570, grad_norm: 1.4161\n",
            "epoch: 5, iteration: 44000, time: 21m 53s, loss: 3.2992, grad_norm: 1.4305\n",
            "epoch: 5, iteration: 45000, time: 22m 22s, loss: 3.1920, grad_norm: 1.4481\n",
            "epoch: 6, iteration: 46000, time: 22m 50s, loss: 3.3794, grad_norm: 1.4196\n",
            "epoch: 6, iteration: 47000, time: 23m 21s, loss: 3.9363, grad_norm: 1.4879\n",
            "epoch: 6, iteration: 48000, time: 23m 50s, loss: 3.2448, grad_norm: 1.4205\n",
            "epoch: 6, iteration: 49000, time: 24m 20s, loss: 3.2374, grad_norm: 1.4237\n",
            "epoch: 6, iteration: 50000, time: 24m 51s, loss: 3.3634, grad_norm: 1.4153\n",
            "epoch: 6, iteration: 51000, time: 25m 21s, loss: 3.2818, grad_norm: 1.4151\n",
            "epoch: 6, iteration: 52000, time: 25m 50s, loss: 3.1799, grad_norm: 1.4287\n",
            "epoch: 6, iteration: 53000, time: 26m 19s, loss: 3.4307, grad_norm: 1.4182\n",
            "epoch: 7, iteration: 54000, time: 26m 48s, loss: 3.2248, grad_norm: 1.4198\n",
            "epoch: 7, iteration: 55000, time: 27m 18s, loss: 3.4741, grad_norm: 1.4149\n",
            "epoch: 7, iteration: 56000, time: 27m 47s, loss: 3.7517, grad_norm: 1.5076\n",
            "epoch: 7, iteration: 57000, time: 28m 17s, loss: 3.2486, grad_norm: 1.4424\n",
            "epoch: 7, iteration: 58000, time: 28m 48s, loss: 3.2202, grad_norm: 1.4149\n",
            "epoch: 7, iteration: 59000, time: 29m 17s, loss: 3.2219, grad_norm: 1.4176\n",
            "epoch: 7, iteration: 60000, time: 29m 47s, loss: 3.1727, grad_norm: 1.4169\n",
            "epoch: 7, iteration: 61000, time: 30m 15s, loss: 3.2850, grad_norm: 1.4166\n",
            "epoch: 8, iteration: 62000, time: 30m 44s, loss: 3.2584, grad_norm: 1.4219\n",
            "epoch: 8, iteration: 63000, time: 31m 14s, loss: 3.3852, grad_norm: 1.4179\n",
            "epoch: 8, iteration: 64000, time: 31m 43s, loss: 3.3078, grad_norm: 1.4240\n",
            "epoch: 8, iteration: 65000, time: 32m 15s, loss: 3.2414, grad_norm: 1.4475\n",
            "epoch: 8, iteration: 66000, time: 32m 44s, loss: 3.4245, grad_norm: 1.4163\n",
            "epoch: 8, iteration: 67000, time: 33m 14s, loss: 3.2900, grad_norm: 1.4160\n",
            "epoch: 8, iteration: 68000, time: 33m 43s, loss: 3.4637, grad_norm: 1.4175\n",
            "epoch: 9, iteration: 69000, time: 34m 11s, loss: 3.3735, grad_norm: 1.4144\n",
            "epoch: 9, iteration: 70000, time: 34m 41s, loss: 3.3664, grad_norm: 1.4186\n",
            "epoch: 9, iteration: 71000, time: 35m 10s, loss: 3.2180, grad_norm: 1.4181\n",
            "epoch: 9, iteration: 72000, time: 35m 41s, loss: 3.2741, grad_norm: 1.4149\n",
            "epoch: 9, iteration: 73000, time: 36m 11s, loss: 3.1377, grad_norm: 1.4207\n",
            "epoch: 9, iteration: 74000, time: 36m 41s, loss: 3.1526, grad_norm: 1.4203\n",
            "epoch: 9, iteration: 75000, time: 37m 10s, loss: 3.4675, grad_norm: 1.4240\n",
            "epoch: 9, iteration: 76000, time: 37m 39s, loss: 3.2094, grad_norm: 1.4149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wicccX-eNZu",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwsFjcYBmWL_",
        "colab_type": "text"
      },
      "source": [
        "We plot the learning curve. We also evaluate the model on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYuprdb3d1i",
        "colab_type": "text"
      },
      "source": [
        "###### **Learning Curve**\n",
        "\n",
        "We plot the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPXOHLheN7G",
        "colab_type": "code",
        "outputId": "becb2914-758c-4478-8096-b4116c2a836c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# plot the learning curve. The x-axis is the training iterations and the y-axis is the training loss. The loss should be going down.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "x_axis_pts = np.array([idx for idx, _ in enumerate(all_losses)])\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  fig, (ax_1, ax_2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax_1.plot(x_axis_pts, all_losses)\n",
        "  ax_1.set_xlabel(\"iteration\")\n",
        "  ax_1.set_ylabel(\"loss\")\n",
        "  ax_1.set_title(\"learning curve\");\n",
        "\n",
        "  ax_2.plot(x_axis_pts, all_grad_norms)\n",
        "  ax_2.set_xlabel(\"iteration\")\n",
        "  ax_2.set_ylabel(\"norm of the gradient\")\n",
        "  ax_2.set_title(\"gradient descent\");\n",
        "\n",
        "else:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x_axis_pts, all_losses)\n",
        "  ax.set_xlabel(\"iteration\")\n",
        "  ax.set_ylabel(\"loss\")\n",
        "  ax.set_title(\"learning curve\");"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eb6fbeb05f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_axis_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb_calc_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P90yPixR3ojS",
        "colab_type": "text"
      },
      "source": [
        "###### **Inference**\n",
        "\n",
        "We use the trained model to run inference on the test dataset and compute the [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [f1 score](https://en.wikipedia.org/wiki/F1_score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVj-JXcOmkBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make prediction for one sentence.\n",
        "def predict(model, sent_tensor):\n",
        "  hidden, memory = model.init_state()\n",
        "\n",
        "  predicted_tag_id = []\n",
        "\n",
        "  for idx in range(sent_tensor.shape[0]):\n",
        "    outputs, hidden, memory = model(sent_tensor[idx].reshape(1, sent_tensor.shape[1]), hidden, memory)\n",
        "    predicted_tag_id.append(np.argmax(outputs.detach().cpu().numpy()))\n",
        "\n",
        "  return predicted_tag_id\n",
        " \n",
        " \n",
        "model.eval()\n",
        "predicted_tags = []\n",
        " \n",
        "for sent_tensor in test_data_oh_list:\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    predicted_tag_id = predict(model, sent_tensor)\n",
        "    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id])\n",
        "  \n",
        "   \n",
        "# precision, recall, and f1 score\n",
        "\n",
        "# Example: true_tag_list/predicted_tag_list:\n",
        "#   [[‘O’, ‘O’, ‘I’, ‘N’, ...]\n",
        "#    [‘I’, ‘I’, ‘O’, ‘N’, ...]],\n",
        "precision, recall, f1_score = util.evaluate_result(test_tags, predicted_tags)\n",
        "print(\"precision: {:0.4f}, recall: {:0.4f}, f1 score: {:0.4f}\".format(precision, recall, f1_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}