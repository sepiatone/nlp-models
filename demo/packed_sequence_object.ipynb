{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo-pytorch-PackedSequence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TZt3LEfPdrCd",
        "J_tXhM73fgf3",
        "_JMn0z3njTbm",
        "MB70-gPHlpRd",
        "m6LpIapkptvm",
        "QzcYe3MdqvTO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kp_yVB0btXe",
        "colab_type": "text"
      },
      "source": [
        "#### **Tutorial on using the PyTorch's PackedSequence object**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppq-dHrhc8iQ",
        "colab_type": "text"
      },
      "source": [
        "Note - This is borrowed from [HarshTrivedi's github demo](https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial). I've recast it as a Jupyter notebook using a GRU.\n",
        "\n",
        "We want to run a GRU on a batch of 3 character sequences [\"long_str\", \"tiny\", \"medium\"]. Here are the steps. You would be interested in *ed steps only.\n",
        "\n",
        "* Construct the vocabulary\n",
        "* Load indexed data (list of instances, where each instance is list of character indices)\n",
        "* Create the model\n",
        "* Pad data **\\***\n",
        "* Sort instances **\\***\n",
        "* Embed the instances **\\***\n",
        "* Call pack_padded_sequence with embeded instances and sequence lengths **\\***\n",
        "* Run the model **\\***\n",
        "* Call unpack_padded_sequences **\\***\n",
        "* Summary of Shape Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkd8vcjMbWEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seqs =  [\"long_str\", \"tiny\", \"medium\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa1igQ5fcko-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZt3LEfPdrCd",
        "colab_type": "text"
      },
      "source": [
        "#### **1. Construct the vocabulary**\n",
        "\n",
        "The vocabulary (a set of tokens) will be the characters in the sequences. We add \"\\<pad\\>\" to represent the padding character / token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d1ve2PSchNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1114463-f63d-46fa-fc74-7bfb276f1d34"
      },
      "source": [
        "# make sure that the index for padding character is 0\n",
        "vocab = [\"<pad>\"] + sorted(set([tok for seq in seqs for tok in seq]))\n",
        "\n",
        "print(\"vocab:\", vocab)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab: ['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_tXhM73fgf3",
        "colab_type": "text"
      },
      "source": [
        "#### **2. Convert the sequences to indexed data**\n",
        "\n",
        "The indexes of each sequence is obtained from the vocabulary, i.e., each sequence becomes a list of character indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCwH8d9of_OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f68ef23-22f8-45d7-eddf-723111206a08"
      },
      "source": [
        "seqs_idx = [ [vocab.index(tok) for tok in seq] for seq in seqs]\n",
        "\n",
        "print(\"seqs_idxs:\", seqs_idx)\n",
        "# seqs_idxs => [[6, 9, 8, 4, 1, 11, 12, 10],\n",
        "#               [12, 5, 8, 14],\n",
        "#               [7, 3, 2, 5, 13, 7]]\n",
        "\n",
        "# print('\\n'.join(' '.join( map(str, lst)) for lst in seqs_idxs))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seqs_idxs: [[6, 9, 8, 4, 1, 11, 12, 10], [12, 5, 8, 14], [7, 3, 2, 5, 13, 7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JMn0z3njTbm",
        "colab_type": "text"
      },
      "source": [
        "#### **3. Create the model**\n",
        "\n",
        "Here we use the GRU model from the PyTorch library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-3Nieyojgh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# create the embedding, embedding dimension = 4\n",
        "embed_dim = 4\n",
        "embed = nn.Embedding(len(vocab), embed_dim)\n",
        "\n",
        "# create the gated recurrent unit with embedded size of each input (input_size) equal to the embedding dimension and with the size of the hidden state = 5\n",
        "input_size = embed_dim\n",
        "hidden_size = 5\n",
        "gru = nn.GRU(input_size, hidden_size, batch_first = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB70-gPHlpRd",
        "colab_type": "text"
      },
      "source": [
        "#### **4. Pad data**\n",
        "\n",
        "We pad the data. The padding character is \"\\<pad\\>\", with index 0. Each sequence is padded up to the maximum length, which is the length of the longest sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZvFhHVlpqK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "26d15f2b-dc9b-4fef-ed9a-a15339cdd784"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "# get the lengths of all the sequences\n",
        "seq_lengths = torch.LongTensor(list(map(len, seqs_idx)))\n",
        "# seq_lengths = [8, 4, 6]\n",
        "# seq_lengths_max = 8\n",
        "\n",
        "seqs_tensor = Variable(torch.zeros( (len(seqs_idx), seq_lengths.max()))).long()\n",
        "# seqs_tensor => [[0 0 0 0 0 0 0 0]\n",
        "#                [0 0 0 0 0 0 0 0]\n",
        "#                [0 0 0 0 0 0 0 0]]\n",
        "\n",
        "for idx, (seq, seq_len) in enumerate(zip(seqs_idx, seq_lengths)):\n",
        "  seqs_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "\n",
        "# seqs_tensor => [[ 6  9  8  4  1 11 12 10]         # long_str\n",
        "#                [12  5  8 14  0  0  0  0]          # tiny\n",
        "#                [ 7  3  2  5 13  7  0  0]]         # medium\n",
        "# seqs_tensor.shape : (batch_size X max_seq_len) = (3 X 8)\n",
        "print(\"seqs_tensor:\", seqs_tensor)\n",
        "print(\"seqs_tensor.shape:\", seqs_tensor.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seqs_tensor: tensor([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
            "        [12,  5,  8, 14,  0,  0,  0,  0],\n",
            "        [ 7,  3,  2,  5, 13,  7,  0,  0]])\n",
            "seqs_tensor.shape: torch.Size([3, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6LpIapkptvm",
        "colab_type": "text"
      },
      "source": [
        "#### **5. Sort instances**\n",
        "\n",
        "We sort the instances by sequence length in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgkMGkxtqJKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_lengths, perm_idx = seq_lengths.sort(0, descending = True)\n",
        "\n",
        "seqs_tensor = seqs_tensor[perm_idx]\n",
        "# seqs_tensor => [[ 6  9  8  4  1 11 12 10]          # long_str\n",
        "#                [ 7  3  2  5 13  7  0  0]           # medium\n",
        "#                [12  5  8 14  0  0  0  0]]          # tiny\n",
        "# seqs_tensor.shape : (batch_size X max_seq_len) = (3 X 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzcYe3MdqvTO",
        "colab_type": "text"
      },
      "source": [
        "#### **6. Embed the instances**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWIoGlFqoHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba403a0b-6656-412c-a3c4-65d3efc9e0e2"
      },
      "source": [
        "seqs_tensor_embedded = embed(seqs_tensor)\n",
        "# seqs_tensor_embedded =>\n",
        "#                       [[[ 0.1114, -0.1977, -0.0224, -1.0467]     l\n",
        "#                         [ 0.7710,  0.2153, -1.3473,  0.8217]     o\n",
        "#                         [-1.8285, -2.1818, -1.5927, -1.7026]     n\n",
        "#                         [-0.8871,  0.2909, -0.0493, -1.0087]     g\n",
        "#                         [ 0.9886,  0.8594, -0.2939, -0.0761]     _\n",
        "#                         [ 0.3097, -1.2243, -0.7324, -0.7734]     s\n",
        "#                         [ 0.0948, -0.1665, -1.0248, -2.0838]     t\n",
        "#                         [-1.1538,  0.7745, -0.0513, -0.1554]]    r\n",
        "\n",
        "#                        [[ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
        "#                         [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     e\n",
        "#                         [ 0.48159844 -1.4886451   0.92639893  0.76906884]     d\n",
        "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
        "#                         [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     u\n",
        "#                         [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]    <pad>\n",
        "\n",
        "#                        [[ 0.64004815  0.45813003  0.3476034  -0.03451729]     t\n",
        "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
        "#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n\n",
        "#                         [-1.284392    0.68294704  1.4064184  -0.42879772]     y\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]]   <pad>\n",
        "# embedded_seq_tensor.shape : (batch_size X max_seq_len X embedding_dim) = (3 X 8 X 4)\n",
        "# print(\"seqs_tensor_embedded:\", seqs_tensor_embedded)\n",
        "print(\"seqs_tensor_embedded.shape:\", seqs_tensor_embedded.shape)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seqs_tensor_embedded.shape: torch.Size([3, 8, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PYFMDWJsErL",
        "colab_type": "text"
      },
      "source": [
        "#### **7. Call pack_padded_sequence**\n",
        "\n",
        "We call pack_padded_sequence() with embeded instances and sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUDSxxSLsTmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "packed_input = pack_padded_sequence(seqs_tensor_embedded, seq_lengths, batch_first = True)\n",
        "\n",
        "# packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
        "#\n",
        "# packed_input.data =>\n",
        "#                         [[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l\n",
        "#                          [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     m\n",
        "#                          [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     t\n",
        "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     o\n",
        "#                          [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     e\n",
        "#                          [-1.284392    0.68294704  1.4064184  -0.42879772]     i\n",
        "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
        "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     d\n",
        "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
        "#                          [-0.23622951  2.0361056   0.15435742 -0.04513785]     g\n",
        "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     i\n",
        "#                          [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    y\n",
        "#                          [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     _\n",
        "#                          [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     u\n",
        "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     s\n",
        "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     m\n",
        "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     t\n",
        "#                          [ 0.48159844 -1.4886451   0.92639893  0.76906884]     r\n",
        "# packed_input.data.shape : (batch_sum_seq_len X embedding_dim) = (18 X 4)\n",
        "#\n",
        "# packed_input.batch_sizes => [ 3,  3,  3,  3,  2,  2,  1,  1]\n",
        "# visualization :\n",
        "# l  o  n  g  _  s  t  r   #(long_str)\n",
        "# m  e  d  i  u  m         #(medium)\n",
        "# t  i  n  y               #(tiny)\n",
        "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])\n",
        "\n",
        "# print(\"packed_input.data:\", packed_input.data)\n",
        "# print(\"packed_input.batch_sizes:\", packed_input.batch_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ajJEdp5vX_w",
        "colab_type": "text"
      },
      "source": [
        "#### **8. Run the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpzqJKBevhSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "packed_output, h_t = gru(packed_input)\n",
        "\n",
        "# packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
        "#\n",
        "# packed_output.data :\n",
        "#                          [[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
        "#                           [ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m\n",
        "#                           [ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t\n",
        "#                           [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o\n",
        "#                           [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e\n",
        "#                           [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i\n",
        "#                           [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n\n",
        "#                           [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d\n",
        "#                           [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n\n",
        "#                           [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g\n",
        "#                           [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i\n",
        "#                           [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y\n",
        "#                           [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _\n",
        "#                           [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u\n",
        "#                           [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s\n",
        "#                           [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m\n",
        "#                           [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t\n",
        "#                           [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r\n",
        "# packed_output.data.shape : (batch_sum_seq_len X hidden_dim) = (18 X 5)\n",
        "\n",
        "# packed_output.batch_sizes => [ 3,  3,  3,  3,  2,  2,  1,  1] (same as packed_input.batch_sizes)\n",
        "# visualization :\n",
        "# l  o  n  g  _  s  t  r   #(long_str)\n",
        "# m  e  d  i  u  m         #(medium)\n",
        "# t  i  n  y               #(tiny)\n",
        "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])\n",
        "\n",
        "# print(\"packed_output.data:\", packed_output.data)\n",
        "# print(\"packed_output.batch_sizes:\", packed_output.batch_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fB9Z0c0w1go",
        "colab_type": "text"
      },
      "source": [
        "#### **9. Call unpack_padded_sequences**\n",
        "\n",
        "We could all just pick the last hidden vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGpzkCfgxUis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3b640393-7b92-4655-a15f-00e95732013e"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "seqs_op, seqs_op_len = pad_packed_sequence(packed_output, batch_first = True)\n",
        "\n",
        "# output:\n",
        "# seqs_op =>\n",
        "#                          [[[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
        "#                            [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o\n",
        "#                            [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n\n",
        "#                            [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g\n",
        "#                            [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _\n",
        "#                            [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s\n",
        "#                            [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t\n",
        "#                            [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r\n",
        "\n",
        "#                           [[ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m\n",
        "#                            [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e\n",
        "#                            [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d\n",
        "#                            [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i\n",
        "#                            [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u\n",
        "#                            [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]]  <pad>\n",
        "\n",
        "#                           [[ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t\n",
        "#                            [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i\n",
        "#                            [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n\n",
        "#                            [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]]] <pad>\n",
        "# seqs_op.shape : ( batch_size X max_seq_len X hidden_dim) = (3 X 8 X 5)\n",
        "# print(\"seqs_op:\", seqs_op)\n",
        "# print(\"seqs_op.shape:\", seqs_op.shape)\n",
        "\n",
        "# Or if you just want the final hidden state?\n",
        "print(\"h_t:\", h_t[-1])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h_t: tensor([[ 0.2999,  0.3441, -0.2094, -0.1904,  0.0267],\n",
            "        [-0.2112, -0.0114,  0.0353,  0.3477, -0.2878],\n",
            "        [ 0.1597,  0.1993,  0.0709, -0.6301, -0.1129]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq3SrP4RyhXn",
        "colab_type": "text"
      },
      "source": [
        "#### **Summary of shape transformations**\n",
        "\n",
        "#### (batch_size X max_seq_len X embedding_dim) --> SORT (by length) ---> (batch_size X max_seq_len X embedding_dim)\n",
        "#### (batch_size X max_seq_len X embedding_dim) --->      PACK     ---> (batch_sum_seq_len X embedding_dim)\n",
        "#### (batch_sum_seq_len X embedding_dim)        --->      GRU     ---> (batch_sum_seq_len X hidden_dim)\n",
        "#### (batch_sum_seq_len X hidden_dim)           --->    UNPACK     ---> (batch_size X max_seq_len X hidden_dim)"
      ]
    }
  ]
}