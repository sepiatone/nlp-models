{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dkDEsdD3Wsda",
        "AU3aenm0Zota",
        "DzQs9WJngfsp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDEsdD3Wsda",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN for Named Entity Recognition**\n",
        "\n",
        "The NLP task [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) is to classify named entity's within a corpus into predefined categories.\n",
        "\n",
        "We explore the use of a recurrent architecture to perform NER. The dataset used is the MIT-Restaurants dataset. The tagging of the dataset is in the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.\n",
        "\n",
        "Note: Framework based off https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GgGlp7JYtiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import util\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\" # use a gpu!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3aenm0Zota",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Read in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRcN9c6leOWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = util.read_file_txt(\"../data/mit_restaurants-train.dat\", type = \"word\")\n",
        "train_tags = util.read_file_txt(\"../data/mit_restaurants-train.tag\", type = \"word\")\n",
        "\n",
        "test_data = util.read_file_txt(\"../data/mit_restaurants-test.dat\", type = \"word\")\n",
        "test_tags = util.read_file_txt(\"../data/mit_restaurants-test.tag\", type = \"word\")\n",
        "\n",
        "print('number of training samples:', len(train_data))\n",
        "print('number of testing samples:',  len(test_data))\n",
        "# print('average sentence length in training data', (np.mean([len(sent) for sent in train_data])))\n",
        "print()\n",
        "\n",
        "print('the first few sentences are:', train_data[0:3])\n",
        "print('and their cp\\'ding named entity sequences are: ', str(train_tags[0:3]))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzQs9WJngfsp",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44uSZVEXg48r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions and more data preprocessing before we move on to implementing our models.\n",
        "\n",
        "# from train data, collect all unique word types as a set and add 'UNK' to it (unseen words in test data will be turned into 'UNK')\n",
        "vocab_set = list(set([word for sent in train_data for word in sent])) + ['UNK']\n",
        "num_vocabs = len(vocab_set)\n",
        "print(\"number of word types (including 'UNK'):\", num_vocabs)\n",
        "print(\"the first couple and last couple of words in the vocabulary set:\", vocab_set[0:2] +  vocab_set[-2:])\n",
        "\n",
        "vocab2id = {v : i for i, v in enumerate(vocab_set)}\n",
        "\n",
        "#  collect all tag (class) types and assign an unique id to each of them. (here there won't be a unseen tag type in test data)\n",
        "tag_set = list(set([tag for tag_seq in train_tags for tag in tag_seq]))\n",
        "num_tags = len(tag_set)\n",
        "print(\"number of tag types:\", num_tags)\n",
        "print()\n",
        "\n",
        "# assign each tag type a unique id, also create the inverse dict of tag2id (required during evaluation)\n",
        "tag2id = {t : i for i, t in enumerate(tag_set)} \n",
        "id2tag = {i : t for t, i in tag2id.items()}\n",
        "\n",
        "# apply one-hot encoding to data.\n",
        "train_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in train_data]\n",
        "# print(\"oh data[0] - len:\", len(train_data_oh_list[0]), \"shape:\", train_data_oh_list[0].shape)\n",
        "\n",
        "# transform tag names into ids\n",
        "train_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in train_tags]\n",
        "# print(\"list len, tag data:\", len(train_tags_id_list))\n",
        "\n",
        "# train_data_oh_list should now be a list of 2d-tensors, each has shape (sent_len, num_vocabs)\n",
        "# Note that to utilize the `shape` attribute, each element in the list should already be a torch tensor.\n",
        "print(\"first sentence has shape: %s\" % str(train_data_oh_list[0].shape))\n",
        "print(\"fifth sentence has shape: %s\" % str(train_data_oh_list[4].shape))\n",
        "\n",
        "# train_tags_id_list is a list of 1d-tensors, each that has shape (sent_len,)\n",
        "print(\"first tag sequence has shape: %s\" % train_tags_id_list[0].shape)\n",
        "print(\"fifth tag sequence has shape: %s\" % train_tags_id_list[4].shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# Apply same conversion to test dataset.\n",
        "test_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in test_data]\n",
        "test_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in test_tags]\n",
        "# print(\"list len, oh test:\", len(test_data_oh_list))\n",
        "# print(\"list len, tag test:\", len(test_tags_id_list))\n",
        "# print(\"first sentence has shape: %s\" % str(test_data_oh_list[0].shape))\n",
        "# print(\"fifth sentence has shape: %s\" % str(test_data_oh_list[4].shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gCDSGGdfeZ",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN**\n",
        "\n",
        "We implement a vanilla RNN from scratch, then train it and evaluate its performance on the NER task.\n",
        "\n",
        "The RNN is implemented using [Elman](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks) units and the $tanh$ non-linear activation function. The input size was set to the vocbulary size of the dataset, the size of the hidden state was set to $128$ and the output size was set to the number of tags in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37BjwIIef15",
        "colab_type": "text"
      },
      "source": [
        "##### **RNN architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_t2jPVDd3bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.hidden = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
        "    self.output = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    \"\"\"\n",
        "    The `forward` function should just perform one step of update and output logits before softmax.\n",
        "    `x` is a 2d-tensor of shape (1, input_size); `hidden` is another 2d-tensor of shape (1, hidden_size), representing the hidden state of\n",
        "    the previous time step.\n",
        "    \"\"\"\n",
        "\n",
        "    combined = torch.cat( (x, hidden), dim = 1)\n",
        "    hidden = torch.tanh(self.hidden(combined))\n",
        "    output = self.output(hidden)\n",
        "\n",
        "    return output, hidden\n",
        "    \n",
        "\n",
        "  def init_state(self):\n",
        "    \"\"\" Use to initialize hidden state everytime before running a sentence. \"\"\"\n",
        "    return torch.zeros(1, self.hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQfXsM0dz4m",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**\n",
        "\n",
        "The function $train\\_one\\_sample$, takes a (sentence tensor, tag tensor) pair as input and does one step of the gradient update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrT97TNWkHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-3\n",
        "rnn_hidden_size = 128\n",
        "\n",
        "\n",
        "model = RNN(input_size = num_vocabs, hidden_size = rnn_hidden_size, output_size = num_tags).to(device)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Run through a sentence, generate output, compute loss, and perform one gradient update\n",
        "Sentence and tag are represented as a 2d-tensor `sent_tensor` and a 1d-tensor `tag_tensor`, respectively.\n",
        "\"\"\"\n",
        "def train_one_sample(model, sent_tensor, tag_tensor):\n",
        "  hidden = model.init_state() # initialize hidden state\n",
        "  loss = 0.00\n",
        "\n",
        "  outputs = torch.zeros( (1, num_tags)).to(device)\n",
        "\n",
        "  for idx in range(sent_tensor.shape[0]):\n",
        "    outputs, hidden = model(sent_tensor[idx].reshape(1, sent_tensor.shape[1]), hidden)\n",
        "    loss = loss + criterion(outputs, torch.LongTensor([tag_tensor[idx]]).to(device))\n",
        "      \n",
        "  loss = loss / len(tag_tensor)   # average the loss over all tags in the sentance\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return outputs, loss.item()\n",
        "\n",
        "\n",
        "# main training loop for the rnn\n",
        "num_epochs = 10\n",
        "iter_count = 0\n",
        "print_every = 2500\n",
        "plot_every = 50\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "b_calc_grad_norm = True # set to true if we want to analyze how the gradient changes over training\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  current_grad_norm = 0\n",
        "  all_grad_norms = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model.train()\n",
        " \n",
        "for idx_epoch in range(num_epochs):\n",
        "  for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    tag_tensor = tag_tensor.to(device)\n",
        "\n",
        "    output, loss = train_one_sample(model, sent_tensor, tag_tensor)\n",
        "    current_loss += loss\n",
        "    \n",
        "    if b_calc_grad_norm:\n",
        "      grad = torch.FloatTensor( (1, 1)).to(device)\n",
        "      \n",
        "      for param in model.parameters():\n",
        "        grad = torch.cat( (grad, torch.reshape(param.grad, (-1, ))))\n",
        "      \n",
        "      current_grad_norm += torch.norm(grad.to(\"cpu\"), 2)\n",
        "\n",
        "\n",
        "    if iter_count % print_every == 0:      \n",
        "      if b_calc_grad_norm:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}, grad_norm: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss, torch.norm(grad.to(\"cpu\"), 2)))\n",
        "      else:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss))\n",
        "\n",
        "\n",
        "    # add current loss avg to list of losses\n",
        "    if iter_count % plot_every == 0 and iter_count > 0:\n",
        "      all_losses.append(current_loss / plot_every)\n",
        "      current_loss = 0\n",
        "      \n",
        "      if b_calc_grad_norm: \n",
        "        all_grad_norms.append(current_grad_norm / plot_every)\n",
        "        current_grad_norm = 0\n",
        "\n",
        "\n",
        "    iter_count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wicccX-eNZu",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwsFjcYBmWL_",
        "colab_type": "text"
      },
      "source": [
        "We plot the learning curve. We also evaluate the model on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYuprdb3d1i",
        "colab_type": "text"
      },
      "source": [
        "###### **Learning Curve**\n",
        "\n",
        "We plot the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPXOHLheN7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the learning curve. The x-axis is the training iterations and the y-axis is the training loss. The loss should be going down.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "x_axis_pts = np.array([idx for idx, _ in enumerate(all_losses)])\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  fig, (ax_1, ax_2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax_1.plot(x_axis_pts, all_losses)\n",
        "  ax_1.set_xlabel(\"iteration\")\n",
        "  ax_1.set_ylabel(\"loss\")\n",
        "  ax_1.set_title(\"learning curve\");\n",
        "\n",
        "  ax_2.plot(x_axis_pts, all_grad_norms)\n",
        "  ax_2.set_xlabel(\"iteration\")\n",
        "  ax_2.set_ylabel(\"norm of the gradient\")\n",
        "  ax_2.set_title(\"gradient descent\");\n",
        "\n",
        "else:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x_axis_pts, all_losses)\n",
        "  ax.set_xlabel(\"iteration\")\n",
        "  ax.set_ylabel(\"loss\")\n",
        "  ax.set_title(\"learning curve\");\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P90yPixR3ojS",
        "colab_type": "text"
      },
      "source": [
        "###### **Inference**\n",
        "\n",
        "We use the trained model to run inference on the test dataset and compute the [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [f1 score](https://en.wikipedia.org/wiki/F1_score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVj-JXcOmkBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make prediction for one sentence.\n",
        "def predict(model, sent_tensor):\n",
        "  hidden = model.initHidden().to(device)\n",
        "\n",
        "  predicted_tag_id = []\n",
        "\n",
        "  for idx in range(sent_tensor.shape[0]):\n",
        "    outputs, _ = model(sent_tensor[idx].reshape(1, sent_tensor.shape[1]), hidden)\n",
        "    predicted_tag_id.append(np.argmax(outputs.detach().cpu().numpy()))\n",
        "\n",
        "  return predicted_tag_id\n",
        " \n",
        " \n",
        "model.eval()\n",
        "predicted_tags = []\n",
        " \n",
        "for sent_tensor in test_data_oh_list:\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    predicted_tag_id = predict(model, sent_tensor)\n",
        "    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id])\n",
        "  \n",
        "   \n",
        "# precision, recall, and f1 score\n",
        "\n",
        "# Example: true_tag_list/predicted_tag_list:\n",
        "#   [[‘O’, ‘O’, ‘I’, ‘N’, ...]\n",
        "#    [‘I’, ‘I’, ‘O’, ‘N’, ...]],\n",
        "precision, recall, f1_score = util.evaluate_result(test_tags, predicted_tags)\n",
        "print(\"precision: {:0.4f}, recall: {:0.4f}, f1 score: {:0.4f}\".format(precision, recall, f1_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}