{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AU3aenm0Zota",
        "DzQs9WJngfsp",
        "m37BjwIIef15"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDEsdD3Wsda",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN for Named Entity Recognition**\n",
        "\n",
        "The NLP task [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) is to classify named entity's within a corpus into predefined categories.\n",
        "\n",
        "We explore the use of a recurrent architecture to perform NER. The dataset used is the MIT-Restaurants dataset. The tagging of the dataset is in the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.\n",
        "\n",
        "Note: Framework based off https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3aenm0Zota",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Import and read in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRcN9c6leOWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d2911ba7-8582-4a46-95bf-cb21a8949208"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import util\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\" # use a gpu!\n",
        "train_data = util.read_file_txt(\"../data/mit_restaurants-train.dat\", type = \"word\")\n",
        "train_tags = util.read_file_txt(\"../data/mit_restaurants-train.tag\", type = \"word\")\n",
        "\n",
        "test_data = util.read_file_txt(\"../data/mit_restaurants-test.dat\", type = \"word\")\n",
        "test_tags = util.read_file_txt(\"../data/mit_restaurants-test.tag\", type = \"word\")\n",
        "\n",
        "print('number of training samples:', len(train_data))\n",
        "print('number of testing samples:',  len(test_data))\n",
        "# print('average sentence length in training data', (np.mean([len(sent) for sent in train_data])))\n",
        "print()\n",
        "\n",
        "print('the first few sentences are:', train_data[0:3])\n",
        "print('and their cp\\'ding named entity sequences are: ', str(train_tags[0:3]))\n",
        "print()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples: 7660\n",
            "number of testing samples: 1521\n",
            "\n",
            "the first few sentences are: [['2', 'start', 'restaurants', 'with', 'inside', 'dining'], ['34'], ['5', 'star', 'resturants', 'in', 'my', 'town']]\n",
            "and their cp'ding named entity sequences are:  [['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity'], ['O'], ['B-Rating', 'I-Rating', 'O', 'B-Location', 'I-Location', 'I-Location']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzQs9WJngfsp",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44uSZVEXg48r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "97db7cb3-6841-44c4-ccd5-2092c467fed1"
      },
      "source": [
        "# helper functions and more data preprocessing before we move on to implementing our models.\n",
        "\n",
        "# from train data, collect all unique word types as a set and add 'UNK' to it (unseen words in test data will be turned into 'UNK')\n",
        "vocab_set = list(set([word for sent in train_data for word in sent])) + ['UNK']\n",
        "num_vocabs = len(vocab_set)\n",
        "print(\"number of word types (including 'UNK'):\", num_vocabs)\n",
        "print(\"the first couple and last couple of words in the vocabulary set:\", vocab_set[0:2] +  vocab_set[-2:])\n",
        "\n",
        "vocab2id = {v : i for i, v in enumerate(vocab_set)}\n",
        "\n",
        "#  collect all tag (class) types and assign an unique id to each of them. (here there won't be a unseen tag type in test data)\n",
        "tag_set = list(set([tag for tag_seq in train_tags for tag in tag_seq]))\n",
        "num_tags = len(tag_set)\n",
        "print(\"number of tag types:\", num_tags)\n",
        "print()\n",
        "\n",
        "# assign each tag type a unique id, also create the inverse dict of tag2id (required during evaluation)\n",
        "tag2id = {t : i for i, t in enumerate(tag_set)} \n",
        "id2tag = {i : t for t, i in tag2id.items()}\n",
        "\n",
        "# apply one-hot encoding to data.\n",
        "train_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in train_data]\n",
        "# print(\"oh data[0] - len:\", len(train_data_oh_list[0]), \"shape:\", train_data_oh_list[0].shape)\n",
        "\n",
        "# transform tag names into ids\n",
        "train_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in train_tags]\n",
        "# print(\"list len, tag data:\", len(train_tags_id_list))\n",
        "\n",
        "# train_data_oh_list should now be a list of 2d-tensors, each has shape (sent_len, num_vocabs)\n",
        "# Note that to utilize the `shape` attribute, each element in the list should already be a torch tensor.\n",
        "print(\"first sentence has shape: %s\" % str(train_data_oh_list[0].shape))\n",
        "print(\"fifth sentence has shape: %s\" % str(train_data_oh_list[4].shape))\n",
        "\n",
        "# train_tags_id_list is a list of 1d-tensors, each that has shape (sent_len,)\n",
        "print(\"first tag sequence has shape: %s\" % train_tags_id_list[0].shape)\n",
        "print(\"fifth tag sequence has shape: %s\" % train_tags_id_list[4].shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# Apply same conversion to test dataset.\n",
        "test_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in test_data]\n",
        "test_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in test_tags]\n",
        "# print(\"list len, oh test:\", len(test_data_oh_list))\n",
        "# print(\"list len, tag test:\", len(test_tags_id_list))\n",
        "# print(\"first sentence has shape: %s\" % str(test_data_oh_list[0].shape))\n",
        "# print(\"fifth sentence has shape: %s\" % str(test_data_oh_list[4].shape))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of word types (including 'UNK'): 3805\n",
            "the first couple and last couple of words in the vocabulary set: ['moderately', 'another', '2', 'UNK']\n",
            "number of tag types: 17\n",
            "\n",
            "first sentence has shape: torch.Size([6, 3805])\n",
            "fifth sentence has shape: torch.Size([12, 3805])\n",
            "first tag sequence has shape: 6\n",
            "fifth tag sequence has shape: 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gCDSGGdfeZ",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN**\n",
        "\n",
        "We implement a vanilla RNN from scratch, then train it and evaluate its performance on the NER task.\n",
        "\n",
        "The RNN is implemented using [Elman](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks) units and the $tanh$ non-linear activation function. The input size was set to the vocbulary size of the dataset, the size of the hidden state was set to $128$ and the output size was set to the number of tags in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37BjwIIef15",
        "colab_type": "text"
      },
      "source": [
        "##### **RNN architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_t2jPVDd3bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_NER(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.rnn_ner = nn.RNN(self.input_size, self.hidden_size)\n",
        "    self.output = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    \"\"\"\n",
        "    The `forward` function should just perform one step of update and output logits before softmax.\n",
        "    `x` is a 2d-tensor of shape (1, input_size); `hidden` is another 2d-tensor of shape (1, hidden_size), representing the hidden state of \n",
        "    the previous time step.\n",
        "    \"\"\"\n",
        "\n",
        "    output_rnn, hidden = self.rnn_ner(x, hidden)\n",
        "    output = self.output(output_rnn)\n",
        "\n",
        "    return output, hidden\n",
        "    \n",
        "\n",
        "  def init_state(self):\n",
        "    \"\"\" Use to initialize hidden state everytime before running a sentence. \"\"\"\n",
        "    return torch.zeros( (1, 1, hidden_size), dtype = torch.float).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQfXsM0dz4m",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**\n",
        "\n",
        "The function $train\\_one\\_sample$, takes a (sentence tensor, tag tensor) pair as input and does one step of the gradient update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b6fe379f-44be-4fe0-e14d-cf56fa737414",
        "id": "e6gEAgJv8lOp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "learning_rate = 1e-3\n",
        "hidden_size = 128\n",
        "\n",
        "\n",
        "model = RNN_NER(input_size = num_vocabs, hidden_size = hidden_size, output_size = num_tags).to(device)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Run through a sentence, generate output, compute loss, and perform one gradient update\n",
        "Sentence and tag are represented as a 2d-tensor `sent_tensor` and a 1d-tensor `tag_tensor`, respectively.\n",
        "\"\"\"\n",
        "def train_one_sample(model, sent_tensor, tag_tensor):\n",
        "  hidden = model.init_state() # initialize hidden state\n",
        "  loss = torch.zeros(1, dtype = torch.float).to(device)\n",
        "\n",
        "  # print(\"sent_tensor.shape:\", sent_tensor.shape)\n",
        "  sent_tensor = sent_tensor.reshape(sent_tensor.shape[0], 1, sent_tensor.shape[1])\n",
        "  # print(\"sent_tensor.shape:\", sent_tensor.shape)\n",
        "\n",
        "  output, _ = model(sent_tensor, hidden)\n",
        "\n",
        "  # print(\"output.shape:\", output.shape, \"tag_tensor.shape:\", tag_tensor.shape)\n",
        "  output= output.reshape(output.shape[0], output.shape[2])\n",
        "  # print(\"output.shape:\", output.shape)\n",
        "\n",
        "  loss = criterion(output, tag_tensor)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return output, loss.item()\n",
        "\n",
        "\n",
        "# main training loop for the rnn\n",
        "num_epochs = 10\n",
        "iter_count = 0\n",
        "print_every = 2000\n",
        "plot_every = 50\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "b_calc_grad_norm = False # set to true if we want to analyze how the gradient changes over training\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  current_grad_norm = 0\n",
        "  all_grad_norms = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model.train()\n",
        " \n",
        "for idx_epoch in range(num_epochs):\n",
        "  for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    tag_tensor = tag_tensor.to(device)\n",
        "\n",
        "    output, loss = train_one_sample(model, sent_tensor, tag_tensor)\n",
        "    current_loss += loss\n",
        "    \n",
        "    if b_calc_grad_norm:\n",
        "      grad = torch.FloatTensor( (1, 1)).to(device)\n",
        "      \n",
        "      for param in model.parameters():\n",
        "        grad = torch.cat( (grad, torch.reshape(param.grad, (-1, ))))\n",
        "      \n",
        "      current_grad_norm += torch.norm(grad.to(\"cpu\"), 2)\n",
        "\n",
        "\n",
        "    if iter_count % print_every == 0:      \n",
        "      if b_calc_grad_norm:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}, grad_norm: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss, torch.norm(grad.to(\"cpu\"), 2)))\n",
        "      else:\n",
        "        print(\"epoch: {}, iteration: {}, time: {}, loss: {:0.4f}\".format(idx_epoch, iter_count, util.time_since(start), loss))\n",
        "\n",
        "\n",
        "    # add current loss avg to list of losses\n",
        "    if iter_count % plot_every == 0 and iter_count > 0:\n",
        "      all_losses.append(current_loss / plot_every)\n",
        "      current_loss = 0\n",
        "      \n",
        "      if b_calc_grad_norm: \n",
        "        all_grad_norms.append(current_grad_norm / plot_every)\n",
        "        current_grad_norm = 0\n",
        "\n",
        "\n",
        "    iter_count += 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, iteration: 0, time: 0m 0s, loss: 2.7993\n",
            "epoch: 0, iteration: 2000, time: 0m 4s, loss: 0.3030\n",
            "epoch: 0, iteration: 4000, time: 0m 8s, loss: 1.4283\n",
            "epoch: 0, iteration: 6000, time: 0m 12s, loss: 0.5682\n",
            "epoch: 1, iteration: 8000, time: 0m 16s, loss: 0.4596\n",
            "epoch: 1, iteration: 10000, time: 0m 20s, loss: 0.1829\n",
            "epoch: 1, iteration: 12000, time: 0m 24s, loss: 0.0254\n",
            "epoch: 1, iteration: 14000, time: 0m 28s, loss: 0.0326\n",
            "epoch: 2, iteration: 16000, time: 0m 33s, loss: 0.4858\n",
            "epoch: 2, iteration: 18000, time: 0m 37s, loss: 0.1184\n",
            "epoch: 2, iteration: 20000, time: 0m 41s, loss: 1.2015\n",
            "epoch: 2, iteration: 22000, time: 0m 45s, loss: 0.1121\n",
            "epoch: 3, iteration: 24000, time: 0m 49s, loss: 0.0189\n",
            "epoch: 3, iteration: 26000, time: 0m 53s, loss: 0.1670\n",
            "epoch: 3, iteration: 28000, time: 0m 57s, loss: 0.0064\n",
            "epoch: 3, iteration: 30000, time: 1m 1s, loss: 0.0323\n",
            "epoch: 4, iteration: 32000, time: 1m 5s, loss: 0.0054\n",
            "epoch: 4, iteration: 34000, time: 1m 9s, loss: 0.0282\n",
            "epoch: 4, iteration: 36000, time: 1m 13s, loss: 0.0526\n",
            "epoch: 4, iteration: 38000, time: 1m 18s, loss: 0.1469\n",
            "epoch: 5, iteration: 40000, time: 1m 22s, loss: 0.5004\n",
            "epoch: 5, iteration: 42000, time: 1m 26s, loss: 0.0515\n",
            "epoch: 5, iteration: 44000, time: 1m 30s, loss: 0.0209\n",
            "epoch: 6, iteration: 46000, time: 1m 34s, loss: 0.7784\n",
            "epoch: 6, iteration: 48000, time: 1m 38s, loss: 0.3598\n",
            "epoch: 6, iteration: 50000, time: 1m 42s, loss: 0.0207\n",
            "epoch: 6, iteration: 52000, time: 1m 46s, loss: 0.3618\n",
            "epoch: 7, iteration: 54000, time: 1m 50s, loss: 0.1009\n",
            "epoch: 7, iteration: 56000, time: 1m 54s, loss: 0.1522\n",
            "epoch: 7, iteration: 58000, time: 1m 58s, loss: 0.0099\n",
            "epoch: 7, iteration: 60000, time: 2m 2s, loss: 0.0087\n",
            "epoch: 8, iteration: 62000, time: 2m 6s, loss: 0.5661\n",
            "epoch: 8, iteration: 64000, time: 2m 10s, loss: 0.0661\n",
            "epoch: 8, iteration: 66000, time: 2m 14s, loss: 0.0277\n",
            "epoch: 8, iteration: 68000, time: 2m 18s, loss: 0.3454\n",
            "epoch: 9, iteration: 70000, time: 2m 22s, loss: 0.0101\n",
            "epoch: 9, iteration: 72000, time: 2m 26s, loss: 0.2233\n",
            "epoch: 9, iteration: 74000, time: 2m 31s, loss: 0.0102\n",
            "epoch: 9, iteration: 76000, time: 2m 35s, loss: 0.0040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wicccX-eNZu",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwsFjcYBmWL_",
        "colab_type": "text"
      },
      "source": [
        "We plot the learning curve. We also evaluate the model on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYuprdb3d1i",
        "colab_type": "text"
      },
      "source": [
        "###### **Learning Curve**\n",
        "\n",
        "We plot the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPXOHLheN7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f6eb7581-c4dc-4d7e-9db1-97ddcda2d167"
      },
      "source": [
        "# plot the learning curve. The x-axis is the training iterations and the y-axis is the training loss. The loss should be going down.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "x_axis_pts = np.array([idx for idx, _ in enumerate(all_losses)])\n",
        "\n",
        "if b_calc_grad_norm:\n",
        "  fig, (ax_1, ax_2) = plt.subplots(1, 2)\n",
        "  \n",
        "  ax_1.plot(x_axis_pts, all_losses)\n",
        "  ax_1.set_xlabel(\"iteration\")\n",
        "  ax_1.set_ylabel(\"loss\")\n",
        "  ax_1.set_title(\"learning curve\");\n",
        "\n",
        "  ax_2.plot(x_axis_pts, all_grad_norms)\n",
        "  ax_2.set_xlabel(\"iteration\")\n",
        "  ax_2.set_ylabel(\"norm of the gradient\")\n",
        "  ax_2.set_title(\"gradient descent\");\n",
        "\n",
        "else:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x_axis_pts, all_losses)\n",
        "  ax.set_xlabel(\"iteration\")\n",
        "  ax.set_ylabel(\"loss\")\n",
        "  ax.set_title(\"learning curve\");\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCoQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P90yPixR3ojS",
        "colab_type": "text"
      },
      "source": [
        "###### **Inference**\n",
        "\n",
        "We use the trained model to run inference on the test dataset and compute the [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [f1 score](https://en.wikipedia.org/wiki/F1_score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVj-JXcOmkBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2acfc66-e76d-4e41-b6c0-1458dfe117b9"
      },
      "source": [
        "# Make prediction for one sentence.\n",
        "def predict(model, sent_tensor):\n",
        "  hidden = model.init_state() # initialize hidden state\n",
        "\n",
        "  predicted_tag_id = []\n",
        "\n",
        "  sent_tensor = sent_tensor.reshape(sent_tensor.shape[0], 1, sent_tensor.shape[1])\n",
        "  output, _ = model(sent_tensor, hidden)\n",
        "  output = output.reshape(output.shape[0], output.shape[2])\n",
        "  predicted_tag_id = np.argmax(output.detach().cpu().numpy(), axis = 1)\n",
        "\n",
        "  return predicted_tag_id\n",
        " \n",
        " \n",
        "model.eval()\n",
        "predicted_tags = []\n",
        " \n",
        "for sent_tensor in test_data_oh_list:\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    predicted_tag_id = predict(model, sent_tensor)\n",
        "    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id])\n",
        "  \n",
        "   \n",
        "# precision, recall, and f1 score\n",
        "\n",
        "# Example: true_tag_list/predicted_tag_list:\n",
        "#   [[‘O’, ‘O’, ‘I’, ‘N’, ...]\n",
        "#    [‘I’, ‘I’, ‘O’, ‘N’, ...]],\n",
        "precision, recall, f1_score = util.evaluate_result(test_tags, predicted_tags)\n",
        "print(\"precision: {:0.4f}, recall: {:0.4f}, f1 score: {:0.4f}\".format(precision, recall, f1_score))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.7357, recall: 0.7396, f1 score: 0.7308\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}