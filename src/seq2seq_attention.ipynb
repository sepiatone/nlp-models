{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XOnt2gswACKB",
        "zDJjmvZfHV_l",
        "kb5gQEp7oVdi",
        "M06QOTbCALGy"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOArV2r9Piz",
        "colab_type": "text"
      },
      "source": [
        "#### **Machine Translation with an attention based Sequence-to-Sequence Model**\n",
        "\n",
        "We explore the Machine Translation (MT) task using RNN-based [sequence-to-sequence (seq2seq)](https://arxiv.org/abs/1409.3215) models with the [attention mechanism](https://arxiv.org/abs/1409.0473). In seq2seq models there is no one-to-one correspondence between the input and output sequence.\n",
        "\n",
        "We will use a Vietnamese-English dataset from IWSLT'15. The task is to translate a Vietnamese sentence into English.\n",
        "\n",
        "The [framework](https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2b.ipynb) for the code was provided by the TAs for MIT 6.864 (Advanced Natural Language Processing), Spring 2020.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOnt2gswACKB",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Import required libraries and read in the dataset.\n",
        "\n",
        "If the dataset is not downloaded, we download it and place it under the \"data\" directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwd4nva1Q9oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import util\n",
        "from util import MTDataset\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu!\n",
        "\n",
        "\n",
        "# util.get_dataset(\"en_vi_iwslt_15\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDJjmvZfHV_l",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**\n",
        "\n",
        "We do some simple data preprocessing and show some data statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfkQGqV30hgC",
        "colab_type": "code",
        "outputId": "8bb1a6ab-de9e-4193-d5a1-6078ee084ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "# Note in the vocab files for English and Vietnamese, each line is a word in the vocabulary,\n",
        "# that's why we set \"type\" to sentence\n",
        "src_vocab_set = util.read_file_txt(os.path.join(\"../data\", \"vocab.vi\"), encoding = None, type = \"sentence\")\n",
        "trg_vocab_set = util.read_file_txt(os.path.join(\"../data\", \"vocab.en\"), encoding = None, type = \"sentence\")\n",
        "\n",
        "train_src_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"train.vi\"))\n",
        "train_trg_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"train.en\"))\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "test_src_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"tst2013.vi\"))\n",
        "test_trg_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"tst2013.en\"))\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = util.filter_data(train_src_sentences_list, train_trg_sentences_list)\n",
        "test_src_sentences_list, test_trg_sentences_list = util.filter_data(test_src_sentences_list, test_trg_sentences_list)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "# Show some data stats\n",
        "print(\"Number of training (src, trg) sentence pairs: %d\", len(train_src_sentences_list))\n",
        "print(\"Number of validation (src, trg) sentence pairs: %d\", len(val_src_sentences_list))\n",
        "print(\"Number of testing (src, trg) sentence pairs: %d\", len(test_src_sentences_list))\n",
        "\n",
        "src_vocab_set = ['<pad>'] + src_vocab_set\n",
        "trg_vocab_set = ['<pad>'] + trg_vocab_set\n",
        "print(\"Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\", len(src_vocab_set))\n",
        "print(\"Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\", len(trg_vocab_set))\n",
        "\n",
        "length = [len(sent) for sent in train_src_sentences_list]\n",
        "print('Training sentence avg. length: %d ', np.mean(length))\n",
        "print('Training sentence length at 95-percentile: %d', np.percentile(length, 95))\n",
        "print(\"\\n\")\n",
        "\n",
        "print('Training sentence length distribution (x-axis is length range and y-axis is count)\\n')\n",
        "plt.hist(length, bins=5)\n",
        "plt.show()\n",
        "\n",
        "print('Example Vietnamese input: ' + str(train_src_sentences_list[1]))\n",
        "print('Its target English output: ' + str(train_trg_sentences_list[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: %d 23322\n",
            "Number of validation (src, trg) sentence pairs: %d 2591\n",
            "Number of testing (src, trg) sentence pairs: %d 233\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d 7710\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d 17192\n",
            "Training sentence avg. length: %d  32.04364977274676\n",
            "Training sentence length at 95-percentile: %d 46.0\n",
            "\n",
            "\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: Riêng tôi , tôi mê ý tưởng này cực kì .\n",
            "Its target English output: For me , I really love this idea .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5gQEp7oVdi",
        "colab_type": "text"
      },
      "source": [
        "#### **Encoder**\n",
        "\n",
        "RNN seq2seq models consists of an encoder RNN and a decoder RNN.\n",
        "Here we implement the attention mechanism between encoder and decoder; the encoder aims to compress the information contained in the entire input sequence into a single vector and pass it to decoder.\n",
        "\n",
        "We start with implementing the encoder, which is just a simple RNN. Here we use the [GRU](https://pytorch.org/docs/stable/nn.html#gru) architecture for the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnwVGDVkoPt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout = 0.00):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training.\n",
        "         Note that for 1-layer RNN this has no effect since dropout only applies to outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super().__init__()\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True, dropout = dropout, bidirectional = False)\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "    \"\"\"\n",
        "\n",
        "    packed_input = pack_padded_sequence(inputs, lengths, batch_first = True, enforce_sorted = False)\n",
        "    outputs, finals = self.rnn(packed_input)\n",
        "    outputs, outputs_len = pad_packed_sequence(outputs, batch_first = True)\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oz3Kc4QKyEP",
        "colab_type": "text"
      },
      "source": [
        "#### **Decoder (with attention mechanism)**\n",
        "\n",
        "A RNN decoder that uses the encoder's last hidden state to initialize its initial hidden state. We implement the decoder with the [Luong attention model](https://arxiv.org/abs/1508.04025).\n",
        "\n",
        "Attention is calculated with another feedforward layer in the decoder. This layer will use the current input and hidden state to create a new vector, which is the same size as the input sequence (in practice, a fixed maximum length). This vector is processed through softmax to create attention weights, which are multiplied by the encoders' outputs to create a new context vector, which is then used to predict the next output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aukBSNHIxFcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "\n",
        "  def __init__(self, method, hidden_size, max_length = MAX_LENGTH):\n",
        "    super().__init__()\n",
        "\n",
        "    self.method = method\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # if self.method == \"general\":\n",
        "    #     self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    # elif self.method == 'concat':\n",
        "    #     self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "    #     self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    seq_len = len(encoder_outputs)\n",
        "\n",
        "    attn_energies = Variable(torch.zeros(seq_len)).to(device) # variable to store attention energies, B x 1 x S \n",
        "\n",
        "    # calculate energies for each encoder output\n",
        "    for idx in range(seq_len):\n",
        "        attn_energies[idx] = self.score(hidden, encoder_outputs[idx])\n",
        "\n",
        "    # normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
        "    return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "  def score_fn(self, hidden, encoder_output):\n",
        "      \n",
        "    if self.method == \"dot\":\n",
        "      score = torch.mm(hidden, encoder_output)\n",
        "      return score\n",
        "    \n",
        "    # elif self.method == 'general':\n",
        "    #   energy = self.attn(encoder_output)\n",
        "    #   energy = hidden.dot(energy)\n",
        "    #   return energy\n",
        "    \n",
        "    # elif self.method == 'concat':\n",
        "    #   energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
        "    #   energy = self.other.dot(energy)\n",
        "    #   return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYT0BlfYUJXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "  \"\"\"An attention-based RNN decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, attention = None, dropout = 0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "        - `attention`: this is your self-defined Attention object. You can\n",
        "            either define an individual class for your Attention and pass it\n",
        "            here or leave `attention` as None and just implement everything\n",
        "            here.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True, dropout = dropout, bidirectional = False)\n",
        "\n",
        "    # attention model\n",
        "    if attn_model != \"None\":\n",
        "      self.attn = Attn(attn_model, hidden_weights)\n",
        "   \n",
        "  def forward(self, inputs, encoder_hiddens, encoder_finals,  src_mask, trg_mask, hidden = None, max_len = None):\n",
        "    \"\"\" Unroll the decoder one step at a time.\n",
        "    \n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor (batch_sz, max_seq_len, embed_sz), representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training)\n",
        "      - `encoder_hiddens`: a 3d-tensor (batch_sz, max_seq_len, hidden_sz), representing the encoder outputs for each decoding step to attend to\n",
        "      - `encoder_finals`: a 3d-tensor (num_enc_layers, batch_sz, hidden_sz), representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden states.\n",
        "      - `src_mask`: a 3d-tensor (batch_sz, 1, max_seq_len), representing the mask for source sentences\n",
        "      - `trg_mask`: a 3d-tensor (batch_sz, 1, max_seq_len), representing the mask for target sentences\n",
        "      - `hidden`: a 3d-tensor (1, batch_sz, hidden_sz), representing the value to be used to initialize the initial decoder hidden states\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: (same as in Decoder) a 3d-tensor (batch_sz, max_seq_len, hidden_sz), representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector)\n",
        "      - `hidden`: a 3d-tensor (1, batch_sz, hidden_sz), representing the last decoder hidden state\n",
        "    \"\"\"\n",
        "\n",
        "    # the maximum number of steps to unroll the rnn\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1) # max_seq_len\n",
        "\n",
        "    # initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs, hidden = self.rnn(inputs, hidden)\n",
        "\n",
        "    attn_weights = self.attn(inputs, encoder_hiddens) # create the attention weights\n",
        "    context = torch.bmm(attn_weights, encoder_finals) \n",
        "\n",
        "    outputs = F.log_softmax( F.tanh(torch.cat(hidden, context)))\n",
        "\n",
        "    return outputs, hidden, attn_weights # return the attn_weights for visualization\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\" use the encoder's final hidden state to initialize decoder's first hidden state\"\"\"\n",
        "    decoder_init_hiddens = encoder_finals\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH0VdHE2_x1k",
        "colab_type": "text"
      },
      "source": [
        "#### **Encoder-Decoder**\n",
        "\n",
        "We define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBaAYB_oHxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderAttentionDecoder(nn.Module):\n",
        "  \"\"\"A Encoder-Decoder architecture with attention.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `AttentionDecoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    _, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    decoder_outputs, decoder_hiddens = self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "    return decoder_outputs, decoder_hiddens\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden = None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M06QOTbCALGy",
        "colab_type": "text"
      },
      "source": [
        "#### **Generator**\n",
        "\n",
        "It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaHdVcF1KPmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias = False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VIpNlKtK8l_",
        "colab_type": "text"
      },
      "source": [
        "#### **Training and Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsXYZVLHC3rq",
        "colab_type": "text"
      },
      "source": [
        "##### **Dataloading**\n",
        "\n",
        "Apply the dataloader to the mt dataset. The dataloader provides a convenient way to iterate through the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJrXO7nCjzBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# note - the size of the training set may be reduced by setting a smaller value for \"sampling\"\n",
        "train_set = util.MTDataset(train_src_sentences_list, src_vocab_set, train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size, num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = util.MTDataset(val_src_sentences_list, src_vocab_set, val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWaiu7wNBX7x",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**\n",
        "\n",
        "The main functions for training, here we use perplexity to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGa-L1qp13q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_loader, model, loss_compute, print_every, start_time):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for idx, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    # print(\"src_ids_BxT:\", src_ids_BxT.shape)\n",
        "    # print(\"trg_ids_BxL:\", trg_ids_BxL.shape)\n",
        "    # print(\"src_lengths_B:\", src_lengths_B.shape)\n",
        "\n",
        "    # _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "    output, _ = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "    # print(\"output.shape:\", output.shape)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:], norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "\n",
        "    if model.training and idx % print_every == 0:\n",
        "      print(\"iteration: {}, time: {}, loss: {:0.4f}\".format(idx, util.time_since(start_time), loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  criterion = nn.NLLLoss(reduction = \"sum\", ignore_index = PAD_INDEX)   # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when computing the loss.\n",
        "  optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "  dev_ppls = []   # keep track of dev perplexity for each epoch\n",
        "  start = time.time()\n",
        "\n",
        "  for idx in range(num_epochs):\n",
        "    print(\"epoch\", idx)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader = train_data_loader, model = model,\n",
        "                          loss_compute = util.SimpleLossCompute(model, criterion, optim),\n",
        "                          print_every = print_every, start_time = start)\n",
        "         \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader = val_data_loader, model = model,\n",
        "                          loss_compute = util.SimpleLossCompute(model, criterion, None),\n",
        "                          print_every = print_every, start_time = start)\n",
        "      \n",
        "      print(\"validation perplexity: {:0.4f}\".format(dev_ppl))\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls\n",
        "\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.00\n",
        "\n",
        "attn_seq2seq = EncoderAttentionDecoder(\n",
        "  encoder=Encoder(embed_size, hidden_size, dropout = dropout),\n",
        "  decoder=AttentionDecoder(embed_size, hidden_size, attention = None, dropout = dropout),\n",
        "  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "attn_dev_ppls = train(attn_seq2seq, num_epochs = 1, learning_rate = 1e-3, print_every = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU72wCMxDo0f",
        "colab_type": "text"
      },
      "source": [
        "#### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-_hByRlDvOp",
        "colab_type": "text"
      },
      "source": [
        "##### **Perplexity**\n",
        "\n",
        "Plot the perplexity graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTApnlT53YvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "util.plot_perplexity(pure_dev_ppls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpL-_MwJOws",
        "colab_type": "text"
      },
      "source": [
        "##### **Decoding**\n",
        "\n",
        "Decode the model output. For simplicity, we use greedy search here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HTqYwy6-yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n",
        "      prob = model.generator(outputs[:, -1])\n",
        "\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.data.item()\n",
        "    output.append(next_word)\n",
        "    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n",
        "\n",
        "  output = np.array(output)\n",
        "\n",
        "  # Cut off everything starting from </s>.\n",
        "  first_eos = np.where(output == EOS_INDEX)[0]\n",
        "  if len(first_eos) > 0:\n",
        "    output = output[:first_eos[0]]\n",
        "  return output\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab):\n",
        "  return [vocab[i] for i in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5go3VxJZKh",
        "colab_type": "text"
      },
      "source": [
        "Print the top 3 examples from the data loader by applying the greedy decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3m4optFrb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "NUM_EXAMPLES = 3\n",
        "SRC_VOCAB_SET = src_vocab_set\n",
        "TRG_VOCAB_SET = trg_vocab_set\n",
        "\n",
        "def print_examples(model, data_loader):\n",
        "  \"\"\"Prints NUM_EXAMPLES. Assumes a batch size of 1.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for idx, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device), max_len = MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    print(\"example \", idx + 1)\n",
        "    print(\"src: \", \" \".join(lookup_words(src_ids, vocab = SRC_VOCAB_SET)))\n",
        "    print(\"trg: \", \" \".join(lookup_words(trg_ids, vocab = TRG_VOCAB_SET)))\n",
        "    print(\"pred: \", \" \".join(lookup_words(result, vocab = TRG_VOCAB_SET)))\n",
        "    print()\n",
        "\n",
        "    if idx == NUM_EXAMPLES - 1:\n",
        "      break\n",
        "\n",
        "\n",
        "example_set = util.MTDataset(val_src_sentences_list, src_vocab_set, val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(val_set, batch_size = 1, num_workers = 1, shuffle = False)\n",
        "\n",
        "# Here we use the validation dataset to print examples.\n",
        "print_examples(attn_seq2seq, example_data_loader)\n",
        "\n",
        "example_set = util.MTDataset(val_src_sentences_list, src_vocab_set, val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(val_set, batch_size=1, num_workers=1, shuffle=False)\n",
        "\n",
        "print_examples(attn_seq2seq, example_data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pTV5PqJtX4",
        "colab_type": "text"
      },
      "source": [
        "##### **BLEU score**\n",
        "\n",
        "Compute the [BLEU](https://en.wikipedia.org/wiki/BLEU) score, a standard measure to evaluate the translation results.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XGQYwHRPyne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def compute_bleu(model, data_loader):\n",
        "  bleu_score = []\n",
        "\n",
        "  model.eval()\n",
        "  for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device), max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    pred = \" \".join(lookup_words(result, vocab = TRG_VOCAB_SET))\n",
        "    targ = \" \".join(lookup_words(trg_ids, vocab = TRG_VOCAB_SET))\n",
        "\n",
        "    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n",
        "\n",
        "  return bleu_score\n",
        "\n",
        "\n",
        "test_set = MTDataset(test_src_sentences_list, SRC_VOCAB_SET, test_trg_sentences_list, TRG_VOCAB_SET, sampling = 1.)\n",
        "test_data_loader = data.DataLoader(test_set, batch_size = 1, num_workers = 8, shuffle = False)\n",
        "\n",
        "print(\"\\nBLEU score: {:0.4f}\".format(np.mean(compute_bleu(pure_seq2seq, test_data_loader))))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}