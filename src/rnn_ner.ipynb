{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AU3aenm0Zota",
        "m37BjwIIef15",
        "KxQfXsM0dz4m",
        "7wicccX-eNZu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDEsdD3Wsda",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN for Named Entity Recognition**\n",
        "\n",
        "The NLP task [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) is to classify named entity's within a corpus into predefined categories.\n",
        "\n",
        "We explore the use of a recurrent architecture to perform NER. The dataset used is the MIT-Restaurants dataset. The tagging of the dataset is in the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.\n",
        "\n",
        "Note: Framework based off https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GgGlp7JYtiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import util\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\" # use a gpu!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3aenm0Zota",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Read in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRcN9c6leOWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "678799b4-8664-4889-8c7c-cc579e8cff2b"
      },
      "source": [
        "train_data = util.read_file_txt(\"../data/mit_restaurants-train.dat\", type = \"word\")\n",
        "train_tags = util.read_file_txt(\"../data/mit_restaurants-train.tag\", type = \"word\")\n",
        "\n",
        "test_data = util.read_file_txt(\"../data/mit_restaurants-test.dat\", type = \"word\")\n",
        "test_tags = util.read_file_txt(\"../data/mit_restaurants-test.tag\", type = \"word\")\n",
        "\n",
        "print('number of training samples:', len(train_data))\n",
        "print('number of testing samples:',  len(test_data))\n",
        "# print('average sentence length in training data', (np.mean([len(sent) for sent in train_data])))\n",
        "print()\n",
        "\n",
        "print('the first few sentences are:', train_data[0:3])\n",
        "print('and their cp\\'ding named entity sequences are: ', str(train_tags[0:3]))\n",
        "print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training samples: 7660\n",
            "number of testing samples: 1521\n",
            "\n",
            "the first few sentences are: [['2', 'start', 'restaurants', 'with', 'inside', 'dining'], ['34'], ['5', 'star', 'resturants', 'in', 'my', 'town']]\n",
            "and their cp'ding named entity sequences are:  [['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity'], ['O'], ['B-Rating', 'I-Rating', 'O', 'B-Location', 'I-Location', 'I-Location']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzQs9WJngfsp",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44uSZVEXg48r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "06a24e7d-aee6-49b7-aa79-3adb9adcc7c3"
      },
      "source": [
        "# helper functions and more data preprocessing before we move on to implementing our models.\n",
        "\n",
        "# from train data, collect all unique word types as a set and add 'UNK' to it (unseen words in test data will be turned into 'UNK')\n",
        "vocab_set = list(set([word for sent in train_data for word in sent])) + ['UNK']\n",
        "num_vocabs = len(vocab_set)\n",
        "print(\"number of word types (including 'UNK'):\", num_vocabs)\n",
        "print(\"the first couple and last couple of words in the vocabulary set:\", vocab_set[0:2] +  vocab_set[-2:])\n",
        "\n",
        "vocab2id = {v : i for i, v in enumerate(vocab_set)}\n",
        "\n",
        "#  collect all tag (class) types and assign an unique id to each of them. (here there won't be a unseen tag type in test data)\n",
        "tag_set = list(set([tag for tag_seq in train_tags for tag in tag_seq]))\n",
        "num_tags = len(tag_set)\n",
        "print(\"number of tag types:\", num_tags)\n",
        "print()\n",
        "\n",
        "# assign each tag type a unique id, also create the inverse dict of tag2id (required during evaluation)\n",
        "tag2id = {t : i for i, t in enumerate(tag_set)} \n",
        "id2tag = {i : t for t, i in tag2id.items()}\n",
        "\n",
        "# apply one-hot encoding to data.\n",
        "train_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in train_data]\n",
        "# print(\"oh data[0] - len:\", len(train_data_oh_list[0]), \"shape:\", train_data_oh_list[0].shape)\n",
        "\n",
        "# transform tag names into ids\n",
        "train_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in train_tags]\n",
        "# print(\"list len, tag data:\", len(train_tags_id_list))\n",
        "\n",
        "# train_data_oh_list should now be a list of 2d-tensors, each has shape (sent_len, num_vocabs)\n",
        "# Note that to utilize the `shape` attribute, each element in the list should already be a torch tensor.\n",
        "print(\"first sentence has shape: %s\" % str(train_data_oh_list[0].shape))\n",
        "print(\"fifth sentence has shape: %s\" % str(train_data_oh_list[4].shape))\n",
        "\n",
        "# train_tags_id_list is a list of 1d-tensors, each that has shape (sent_len,)\n",
        "print(\"first tag sequence has shape: %s\" % train_tags_id_list[0].shape)\n",
        "print(\"fifth tag sequence has shape: %s\" % train_tags_id_list[4].shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# Apply same conversion to test dataset.\n",
        "test_data_oh_list = [util.one_hot_encoding(sent, vocab2id, vocab_set) for sent in test_data]\n",
        "test_tags_id_list = [util.encoding_idx(tag_seq, tag2id) for tag_seq in test_tags]\n",
        "# print(\"list len, oh test:\", len(test_data_oh_list))\n",
        "# print(\"list len, tag test:\", len(test_tags_id_list))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of word types (including 'UNK'): 3805\n",
            "the first couple and last couple of words in the vocabulary set: ['nouvelle', 'kfc', 'katachi', 'UNK']\n",
            "number of tag types: 17\n",
            "\n",
            "first sentence has shape: torch.Size([6, 3805])\n",
            "fifth sentence has shape: torch.Size([12, 3805])\n",
            "first tag sequence has shape: 6\n",
            "fifth tag sequence has shape: 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gCDSGGdfeZ",
        "colab_type": "text"
      },
      "source": [
        "#### **RNN**\n",
        "\n",
        "We implement a vanilla RNN from scratch, then train it and evaluate its performance on the NER task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37BjwIIef15",
        "colab_type": "text"
      },
      "source": [
        "##### **RNN architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_t2jPVDd3bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    # A torch module implementing an RNN. The `forward` function should just\n",
        "    # perform one step of update and output logits before softmax.\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        # `input_size`, `hidden_size`, and `output_size` are all int.\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
        "        self.output = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # `x` is a 2d-tensor of shape (1, input_size); `hidden` is another\n",
        "        # 2d-tensor of shape (1, hidden_size), representing the hidden state of\n",
        "        # the previous time step.\n",
        "\n",
        "        # output = torch.zeros( (1, self.output_size))        \n",
        "        # print(x.shape, hidden.shape)\n",
        "        combined = torch.cat( (x, hidden), dim = 1)\n",
        "        hidden = self.hidden(combined)\n",
        "        output = self.output(hidden)\n",
        "        \n",
        "        # print(output.shape, hidden.shape)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "\n",
        "    def initHidden(self):\n",
        "        # Use to initialize hidden state everytime before running a sentence.\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQfXsM0dz4m",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrT97TNWkHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Now that you have defined your RNN model, we can start training it.\n",
        "# We've provided the main training loop, but you will have to implement the fucntion `rnn_train_one_sample`,\n",
        "# which takes a (sentence-tensor, tag-tensor)-pair as input and does one step of gradient update.\n",
        "# To understand better what this function is supposed to do, you can go over the main training loop in the next section first.\n",
        "\n",
        "\n",
        "learning_rate = 1e-3\n",
        "rnn_hidden_size = 128\n",
        "\n",
        "\n",
        "rnn_model = RNN(input_size = num_vocabs, hidden_size = rnn_hidden_size, output_size = num_tags).to(device)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "# Run through a sentence, generate output, compute loss, and perform one gradient update\n",
        "# Sentence and tag are represented as a 2d-tensor `sent_tensor` and a 1d-tensor `tag_tensor`, respectively.\n",
        "def rnn_train_one_sample(model, sent_tensor, tag_tensor):\n",
        "    hidden = model.initHidden().to(device)     # initialize hidden state\n",
        "    loss = 0.00 # torch.zeros( (1, 1))\n",
        "\n",
        "    outputs = torch.zeros( (1, num_tags))\n",
        "    \n",
        "    for idx in range(sent_tensor.shape[0]):\n",
        "        outputs, hidden = model(sent_tensor[idx].reshape(1, sent_tensor.shape[1]), hidden)\n",
        "        loss = loss + criterion(outputs, torch.LongTensor([tag_tensor[idx]]))\n",
        "        \n",
        "    loss = loss / len(tag_tensor)   # average the loss over all tags in the sentance\n",
        "\n",
        "    rnn_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    rnn_optimizer.step()\n",
        "\n",
        "    return outputs, loss.item()\n",
        "\n",
        "\n",
        "# main training loop for the rnn\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "n_epochs = 2 # 5\n",
        "iter_count = 0\n",
        "print_every = 100 # 1000\n",
        "plot_every = 50\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "rnn_model.train()\n",
        " \n",
        "for epoch_i in range(n_epochs):\n",
        "    for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n",
        "        sent_tensor = sent_tensor.to(device)\n",
        "        tag_tensor = tag_tensor.to(device)\n",
        "    \n",
        "        output, loss = rnn_train_one_sample(rnn_model, sent_tensor, tag_tensor)\n",
        "        current_loss += loss\n",
        "  \n",
        "        if iter_count % print_every == 0:\n",
        "            print('%d %d %s %.4f' % (n_epochs, iter_count, timeSince(start), loss)) # print('%d %s %.4f' % (iter_count, timeSince(start), loss))\n",
        "  \n",
        "        # add current loss avg to list of losses\n",
        "        if iter_count % plot_every == 0 and iter_count > 0:\n",
        "            all_losses.append(current_loss / plot_every)\n",
        "            current_loss = 0\n",
        "  \n",
        "        iter_count += 1\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wicccX-eNZu",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPXOHLheN7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the learning curve. The x-axis is the training iterations and the y-axis is the training loss. The loss should be going down.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(all_losses)\n",
        "\n",
        "\n",
        "# evaluation / inference\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "# Example: true_tag_list/predicted_tag_list:\n",
        "#   [[‘O’, ‘O’, ‘I’, ‘N’, ...]\n",
        "#    [‘I’, ‘I’, ‘O’, ‘N’, ...]],\n",
        "# each sublist corresponds to an input sentence.\n",
        "def evaluate_result(true_tag_list, predicted_tag_list):\n",
        "    p_list = []\n",
        "    r_list = []\n",
        "    f1_list = []\n",
        "    \n",
        "    for true_tag, predicted_tag in zip(true_tag_list, predicted_tag_list):\n",
        "        p, r, f1, _ = precision_recall_fscore_support(true_tag, predicted_tag,\n",
        "                                                      average='macro',\n",
        "                                                      zero_division=0)\n",
        "        p_list.append(p)\n",
        "        r_list.append(r)\n",
        "        f1_list.append(f1)\n",
        "    \n",
        "    return np.mean(p_list), np.mean(r_list), np.mean(f1_list)\n",
        "\n",
        "\n",
        "# Make prediction for one sentence.\n",
        "def rnn_predict_one_sent(model, sent_tensor):\n",
        "    hidden = model.initHidden().to(device)\n",
        " \n",
        "    predicted_tag_id = None\n",
        "    # Your code here!\n",
        " \n",
        "    return predicted_tag_id\n",
        " \n",
        " \n",
        "rnn_model.eval()\n",
        "predicted_tags = []\n",
        " \n",
        "for sent_tensor in test_data_oh_list:\n",
        "    sent_tensor = sent_tensor.to(device)\n",
        "    predicted_tag_id = rnn_predict_one_sent(rnn_model, sent_tensor)\n",
        "    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id.detach().cpu().numpy()])\n",
        "  \n",
        "   \n",
        "# precision, recall, and f1 score.\n",
        "evaluate_result(test_tags, predicted_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}