{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864_hw4_student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-DF0Cqik3U",
        "colab_type": "text"
      },
      "source": [
        "# **Question-Answering agent**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD3RoMOUixsl",
        "colab_type": "text"
      },
      "source": [
        "### **Introduction**\n",
        "\n",
        "We build a reading comprehension question-and-answer agent.\n",
        "\n",
        "We use a pretrained model. We chose a BERT type model - DistilBERT (a smaller cousin of BERT that uses fewer layers).\n",
        " They are basically the same save for DistilBERT having fewer layers. See this [paper](https://arxiv.org/pdf/1910.01108.pdf) for more info.\n",
        "\n",
        "We need to do the following\n",
        "\n",
        "1. Download a pretrained DistilBERT.\n",
        "2. Add a task-specific readout for Q-and-A. In this case, a linear readout.\n",
        "3. Finetune both DistilBERT and readout for the Q-and-A task.\n",
        "4. When prompted to generate answers, use a sampling algorithm.\n",
        "\n",
        "Note that the actual fine-tuning has already been done and we'll load the fine-tuned model and start form there.\n",
        "\n",
        "You will be asked to understand what the model outputs, and generate human-readable answers to questions.\n",
        "\n",
        "**Since this is the last homework, there are places where little information is given; you are encouraged to learn about the classes and objects by exploring them or seeking documentations online. Still, feel free to post to Piazza if you there's something you don't understand**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x30e6Rym3fna",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Setup**\n",
        "\n",
        "Dataset - The dataset we will be using is the [Stanford Question and Answer Dataset (SQuAD) v1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/).\n",
        "\n",
        "\n",
        "The dataset will be downloaded as `dev-v1.1.json`. This is the evaluation dataset; since you won't be doing the actual finetuning, the training dataset is not needed.\n",
        "\n",
        "(Pretrained) model - We will use hugging face's [transformers](https://huggingface.co/transformers/) package. This package provides a wide variety of pretrained encoder models.\n",
        "You will download the finetuned model separately.\n",
        "\n",
        "\n",
        "Important: Using Google Drive\n",
        "It is highly recommended that you mount your Google Drive to Colab. The code provided to you assumes that you've already done that. Create a folder named `6864_hw4` in your Google Drive root directory and use the code below to mount it. The code should save everything (dataset, feature-ized data, trained models etc.) in the `6864_hw4` folder in your drive.\n",
        "\n",
        "**Nothing for you to code for now, but please understand the lines and get ready to answer some questions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9XgmP3EieYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistics #1: mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "%%bash\n",
        "# Logistics #2: install the transformers package, create a folder, download the dataset and a patch\n",
        "pip -q install transformers\n",
        "\n",
        "# remove the directory if necessary\n",
        "# rm -rf \"/content/gdrive/My Drive/6864_hw4/\"\n",
        "\n",
        "mkdir \"/content/gdrive/My Drive/6864_hw4/\"\n",
        "cd \"/content/gdrive/My Drive/6864_hw4/\"\n",
        "wget -nv -c https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
        "wget -nv -c https://raw.githubusercontent.com/allenai/bi-att-flow/master/squad/evaluate-v1.1.py\n",
        "\n",
        "# fixing an incompatibility between the huggingface package and colab\n",
        "wget -nv -c https://raw.githubusercontent.com/hzshan/mit6864/master/processor.py\n",
        "\n",
        "# download the finetuned model\n",
        "wget -nv -c https://raw.githubusercontent.com/hzshan/mit6864/master/config.json\n",
        "wget -nv -c https://raw.githubusercontent.com/hzshan/mit6864/master/vocab.txt\n",
        "wget -nv -c --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V43WEqmkcH4VP7CDdMkYzITwc_lcSjvP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1V43WEqmkcH4VP7CDdMkYzITwc_lcSjvP\" -O pytorch_model.bin && rm -rf /tmp/cookies.txt\n",
        "\n",
        "\n",
        "import glob, logging, os, random, timeit, torch, sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "\n",
        "from transformers import squad_convert_examples_to_features, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits, squad_evaluate\n",
        "\n",
        "from transformers.data.processors.squad import SquadResult\n",
        "\n",
        "sys.path.append('/content/gdrive/My Drive/6864_hw4')\n",
        "from processor import SquadV1Processor\n",
        "\n",
        "# Make sure you are using GPU as a hardware accelerator for this notebook\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Q0Fe36klmV",
        "colab_type": "text"
      },
      "source": [
        "### **Utility functions**\n",
        "\n",
        "The transformers package provides most of the useful utility functions we need to preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqvbVsNrl8cR",
        "colab_type": "text"
      },
      "source": [
        "We will set up the model, which already combines DistilBERT with a Q-and-A specific decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlUD3x_Hl8tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIR = \"/content/gdrive/My Drive/6864_hw4/\"\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "# Load a trained model and vocabulary that has been fine-tuned by the TAs\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(DIR, do_lower_case=True)\n",
        "model=model.to(device)\n",
        "\n",
        "\n",
        "# Define some parameters\n",
        "max_seq_length = 384\n",
        "doc_stride = 128 # The maximum total input sequence length after WordPiece tokenization.\n",
        "max_query_length = 64 # The maximum number of tokens for the question.\n",
        "batch_size = 8\n",
        "predict_file = 'dev-v1.1.json' # name of the evaluation dataset file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz_gSLF-pcbm",
        "colab_type": "text"
      },
      "source": [
        "### **Convert the dataset to features**\n",
        "\n",
        "Convert the dataset (the .json file contains human-readable texts) to features.\n",
        "\n",
        "If you are doing this for the first time, it may take a few minutes. Afterwards, it will save the featurized dataset in the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rjjtNLBpbYL",
        "colab_type": "code",
        "outputId": "1c5f9d04-4f3a-4181-bae9-266b644e44cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def load_and_cache_examples(tokenizer, evaluate=False, output_examples=False):\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = DIR\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, 'distilbert-base-uncased'.split(\"/\"))).pop(),\n",
        "            str(max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        print('Existing cached file found.')\n",
        "\n",
        "        print('Loading cached features. This may take up to a few minutes.')\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "\n",
        "        processor = SquadV1Processor()\n",
        "        if evaluate:\n",
        "            examples = processor.get_dev_examples(DIR, filename=predict_file)\n",
        "        else:\n",
        "            examples = processor.get_train_examples(DIR, filename=train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=1,\n",
        "        )\n",
        "\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "  \n",
        "\n",
        "dataset, examples, features = load_and_cache_examples(tokenizer, evaluate=True, output_examples=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing input data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 10570/10570 [01:15<00:00, 139.13it/s]\n",
            "add example index and unique id: 100%|██████████| 10570/10570 [00:00<00:00, 759712.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkNchm4cq0j4",
        "colab_type": "text"
      },
      "source": [
        "Here, examples contains text examples of the task; features are the representations fed to the transformers. Explore the various objects and **answer the following questions**.\n",
        "\n",
        "* 3.1. In the Q-and-A model, the encoder is the DistilBERT. What is the decoder? (Hint: look at architecture of the `model` object)\n",
        "\n",
        "* 3.2. What type of Q-and-A task is in the SQuAD dataset? What is provided, what are the queries, and what are the expected answers? (e.g., is it info retrieval or extraction? Are questions related to a text or abstractive? Does the model need to figure out where to look for answers? Does the model need to perform reasoning in order to answer?)\n",
        "* 3.3. In class, we discussed various strategies to build a Q-and-A agent. For example, one strategy to answer questions about a paragraph is to append each question to the paragraph, and ask the model to predict the words that come after the question (as in language modeling). \n",
        "\n",
        "    (1) What is the strategy used here? \n",
        "\n",
        "    (2) Why does it suit the SQuAD dataset? \n",
        "\n",
        "    (3) Describe a Q-and-A task where this wouldn't work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-hBlJOuZwW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Sampling from a trained model**\n",
        "\n",
        "**Once you've answered these questions. Get ready to code!**\n",
        "\n",
        "You will code a sampling algorithm that returns the k-best answers to each question. To do that, a barebone structure has been provided. \n",
        "\n",
        "Before you proceed, you need to understand outputs of the network. We assume that the correct answer is a span in a 384-long string. All we need to do is find out where the answer begins and ends. `outputs` of `model` is a tuple of two 384-long arrays. The first encodes the log likelihood of each word being the start of the answer; the second, the end. \n",
        "\n",
        "In addition, it is good to know that each question in the SQuAD dataset has a unique ID. A code has been provided to you below such that for every question in the evaluation dataset, ID of the question, and the start logits and end logits are combined into a `SquadResult` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwrwN7sftjoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [NOTHING TO CODE IN THIS CELL]\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "eval_sampler = SequentialSampler(dataset)\n",
        "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=batch_size)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# evaluate the data in batches\n",
        "for batch in eval_dataloader:\n",
        "\n",
        "    # set the model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # move batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "\n",
        "        example_indices = batch[3]\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    for i, example_index in enumerate(example_indices):\n",
        "        eval_feature = features[example_index.item()]\n",
        "        unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "        output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "        start_logits, end_logits = output\n",
        "        result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "        all_results.append(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht4VcTIW3ZH5",
        "colab_type": "text"
      },
      "source": [
        "**You will now need to code the following**. \n",
        "* For each question, find the 10 spans that are most likely the answer. \n",
        "* After that, convert them back into texts. \n",
        "* Give a few examples of questions and their corresponding answers. \n",
        "\n",
        "HINT: start by exploring the attributes of `SquadResult`.\n",
        "\n",
        "A note about converting indices to text:\n",
        "A function has been provided below to aid you. Basically, we can tokenize the text in two ways. For example, from \"John Smith's\", we can either can do whitespace tokens (\"John, Smith's\", i.e. `orig_text` below), or the WordPiece tokenizer (\"john smith\" `tok_text` below). The model is trained using the second tokenizer, which would return texts that are different from the original ones. We would like to align the two answers to get the best test (\"John Smith\"). The `get_final_text` function does exactly that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru24ZrLlxAks",
        "colab_type": "code",
        "outputId": "fe308632-824b-4745-c3d2-d291a65999c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "from transformers.data.metrics.squad_metrics import get_final_text\n",
        "import collections\n",
        "\n",
        "def indices_to_text(START_IND, END_IND, feature, example, tokenizer):\n",
        "\n",
        "  \"\"\"feature is the featurized representation of each text, where as eample contains the text in text form\"\"\"\n",
        "\n",
        "  tok_tokens = feature.tokens[START_IND : (END_IND + 1)]\n",
        "  orig_doc_start = feature.token_to_orig_map[START_IND]\n",
        "  orig_doc_end = feature.token_to_orig_map[END_IND]\n",
        "  orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
        "\n",
        "  tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "\n",
        "  # Clean whitespace\n",
        "  tok_text = tok_text.strip()\n",
        "  tok_text = \" \".join(tok_text.split())\n",
        "  orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "  text = get_final_text(tok_text, orig_text, do_lower_case=True)\n",
        "  return text\n",
        "\n",
        "\n",
        "# associate question features to results\n",
        "example_index_to_features = collections.defaultdict(list)\n",
        "for feature in features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "# associate question IDs to results\n",
        "unique_id_to_result = {}\n",
        "for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "for (example_index, example) in enumerate(examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "\n",
        "        # feature.unique_id cantains the unique_id associated with the question. Use it to find corresponding output in `unique_id_to_result`\n",
        "        #[YOUR CODE HERE]\n",
        "\n",
        "        # From outputs of the network, find the start indices and end indices with the highest likelihood\n",
        "        #[YOUR CODE HERE]\n",
        "\n",
        "        result = unique_id_to_result[feature.unique_id]\n",
        "        start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "        end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "\n",
        "        # Search through all possible combinations between the high-likelihood start indices and end indices.\n",
        "        # Remember to exclude invalid ones (e.g. if the end index is smaller than the start). What are other cases where\n",
        "        # an answer is invalid?\n",
        "  \n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Some exclusions are written for you. More are needed\n",
        "                if start_index not in feature.token_to_orig_map:\n",
        "                    continue\n",
        "                if end_index not in feature.token_to_orig_map:\n",
        "                    continue\n",
        "                if not feature.token_is_max_context.get(start_index, False):\n",
        "                    continue\n",
        "\n",
        "                # [YOUR CODE HERE] to exclude other invalid cases\n",
        "\n",
        "                length = end_index - start_index + 1\n",
        "                if length > max_answer_length:\n",
        "                    continue\n",
        "                \n",
        "                # IF the answer is valid, store the start and end indices in order to retrieve the text.\n",
        "                # [YOUR CODE HERE]\n",
        "\n",
        "                # Once you've found the start and end indices that correspond to most likely answers, retreieve the span in text\n",
        "                # [YOUR CODE HERE]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4c9cd2709a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_id_to_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mstart_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_best_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_best_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mend_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_best_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_best_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_get_best_indexes' is not defined"
          ]
        }
      ]
    }
  ]
}