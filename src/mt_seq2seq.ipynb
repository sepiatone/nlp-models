{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mt_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "15vgPnuxKM2A"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOArV2r9Piz",
        "colab_type": "text"
      },
      "source": [
        "#### **Machine Translation with a Sequence-to-Sequence Model**\n",
        "\n",
        "We explore the Machine Translation (MT) task using RNN-based sequence-to-sequence (seq2seq) models.\n",
        "\n",
        "We will use a Vietnamese-English dataset from IWSLT'15. The task is to translate a Vietnamese sentence into English.\n",
        "\n",
        "The [framework](https://github.com/lingo-mit/6864-hw2/blob/master/6864_hw2b.ipynb) for the code was provided by the TAs for MIT 6.864 (Advanced Natural Language Processing), Spring 2020.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15vgPnuxKM2A",
        "colab_type": "text"
      },
      "source": [
        "#### **Setup**\n",
        "\n",
        "Import required libraries and read in the dataset.\n",
        "\n",
        "If the dataset is not downloaded, we download it and place it under the \"data\" directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwd4nva1Q9oC",
        "colab_type": "code",
        "outputId": "257b5431-53c2-4247-bd2b-0d7b22ce6bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "!pip install sacrebleu\n",
        "import sacrebleu\n",
        "\n",
        "import util\n",
        "from util import MTDataset\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu!\n",
        "\n",
        "\n",
        "# util.get_dataset(\"en_vi_iwslt_15\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-1.7.0 sacrebleu-1.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDJjmvZfHV_l",
        "colab_type": "text"
      },
      "source": [
        "#### **Data Preprocessing**\n",
        "\n",
        "We do some simple data preprocessing and show some data statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfkQGqV30hgC",
        "colab_type": "code",
        "outputId": "9a5c473f-f679-4405-e116-7c4a5d3f3b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "# Note in the vocab files for English and Vietnamese, each line is a word in the vocabulary,\n",
        "# that's why we set \"type\" to sentence\n",
        "src_vocab_set = util.read_file_txt(os.path.join(\"../data\", \"vocab.vi\"), encoding = None, type = \"sentence\")\n",
        "trg_vocab_set = util.read_file_txt(os.path.join(\"../data\", \"vocab.en\"), encoding = None, type = \"sentence\")\n",
        "\n",
        "train_src_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"train.vi\"))\n",
        "train_trg_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"train.en\"))\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "test_src_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"tst2013.vi\"))\n",
        "test_trg_sentences_list = util.read_file_txt(os.path.join(\"../data\", \"tst2013.en\"))\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = util.filter_data(train_src_sentences_list, train_trg_sentences_list)\n",
        "test_src_sentences_list, test_trg_sentences_list = util.filter_data(test_src_sentences_list, test_trg_sentences_list)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "# Show some data stats\n",
        "print(\"Number of training (src, trg) sentence pairs: %d\", len(train_src_sentences_list))\n",
        "print(\"Number of validation (src, trg) sentence pairs: %d\", len(val_src_sentences_list))\n",
        "print(\"Number of testing (src, trg) sentence pairs: %d\", len(test_src_sentences_list))\n",
        "\n",
        "src_vocab_set = ['<pad>'] + src_vocab_set\n",
        "trg_vocab_set = ['<pad>'] + trg_vocab_set\n",
        "print(\"Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\", len(src_vocab_set))\n",
        "print(\"Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\", len(trg_vocab_set))\n",
        "\n",
        "length = [len(sent) for sent in train_src_sentences_list]\n",
        "print('Training sentence avg. length: %d ', np.mean(length))\n",
        "print('Training sentence length at 95-percentile: %d', np.percentile(length, 95))\n",
        "print(\"\\n\")\n",
        "\n",
        "print('Training sentence length distribution (x-axis is length range and y-axis is count)\\n')\n",
        "plt.hist(length, bins=5)\n",
        "plt.show()\n",
        "\n",
        "print('Example Vietnamese input: ' + str(train_src_sentences_list[1]))\n",
        "print('Its target English output: ' + str(train_trg_sentences_list[1]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: %d 23322\n",
            "Number of validation (src, trg) sentence pairs: %d 2591\n",
            "Number of testing (src, trg) sentence pairs: %d 233\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d 7710\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d 17192\n",
            "Training sentence avg. length: %d  32.04364977274676\n",
            "Training sentence length at 95-percentile: %d 46.0\n",
            "\n",
            "\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: Riêng tôi , tôi mê ý tưởng này cực kì .\n",
            "Its target English output: For me , I really love this idea .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5gQEp7oVdi",
        "colab_type": "text"
      },
      "source": [
        "#### **Encoder**\n",
        "\n",
        "RNN seq2seq models consists of an encoder RNN and a decoder RNN.\n",
        "In a vanilla seq2seq model where there is no attention mechanism between encoder and decoder, the encoder aims to compress the information contained in the entire input sequence into a single vector and pass it to decoder.\n",
        "\n",
        "We start with implementing the encoder, which is just a simple RNN. Here we use the [GRU](https://pytorch.org/docs/stable/nn.html#gru) architecture for the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnwVGDVkoPt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout = 0.00):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training.\n",
        "         Note that for 1-layer RNN this has no effect since dropout only applies to outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super().__init__()\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True, dropout = dropout, bidirectional = False)\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "    \"\"\"\n",
        "    # print(\"inputs.shape:\", inputs.shape)\n",
        "    # print(\"lengths.shape:\", lengths.shape)\n",
        "\n",
        "    packed_input = pack_padded_sequence(inputs, lengths, batch_first = True, enforce_sorted = False)\n",
        "    outputs, finals = self.rnn(packed_input)\n",
        "    outputs, outputs_len = pad_packed_sequence(outputs, batch_first = True)\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oz3Kc4QKyEP",
        "colab_type": "text"
      },
      "source": [
        "#### **Decoder**\n",
        "\n",
        "A RNN decoder that uses the encoder's last hidden state to initialize its initial hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYT0BlfYUJXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout = 0.00):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True, dropout = dropout, bidirectional = False)\n",
        "\n",
        "    \n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len = None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs, hidden = self.rnn(inputs, hidden)\n",
        "\n",
        "    # return hidden, outputs\n",
        "    return outputs, hidden\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\" Use encoder final hidden state to initialize decoder's first hidden state.\"\"\"\n",
        "    decoder_init_hiddens = encoder_finals\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH0VdHE2_x1k",
        "colab_type": "text"
      },
      "source": [
        "#### **Encoder-Decoder**\n",
        "\n",
        "We define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBaAYB_oHxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    _, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    # trg_ids_new = trg_ids[:, :-1]\n",
        "    # print(\"trg_ids:\", trg_ids.shape, \"trg_ids_new\", trg_ids_new.shape)\n",
        "    decoder_outputs, decoder_hiddens = self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "    return decoder_outputs, decoder_hiddens\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden = None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M06QOTbCALGy",
        "colab_type": "text"
      },
      "source": [
        "#### **Generator**\n",
        "\n",
        "It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaHdVcF1KPmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias = False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VIpNlKtK8l_",
        "colab_type": "text"
      },
      "source": [
        "#### **Training and Testing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmHOPtB8VKwS",
        "colab_type": "text"
      },
      "source": [
        "##### **Dataloading**\n",
        "\n",
        "Apply the dataloader to the dataset. The dataloader provides a convenient way to iterate through the whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr_kz4amVFgX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJrXO7nCjzBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# note - the size of the training set may be reduced by setting a smaller value for \"sampling\"\n",
        "train_set = util.MTDataset(train_src_sentences_list, src_vocab_set, train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size, num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = util.MTDataset(val_src_sentences_list, src_vocab_set, val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWaiu7wNBX7x",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**\n",
        "\n",
        "The main functions for training, here we use perplexity to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGa-L1qp13q",
        "colab_type": "code",
        "outputId": "979253ea-54c1-4144-d378-18472259eff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "def run_epoch(data_loader, model, loss_compute, print_every, start_time):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for idx, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    # print(\"src_ids_BxT:\", src_ids_BxT.shape)\n",
        "    # print(\"trg_ids_BxL:\", trg_ids_BxL.shape)\n",
        "    # print(\"src_lengths_B:\", src_lengths_B.shape)\n",
        "\n",
        "    # _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "    output, _ = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "    # print(\"output.shape:\", output.shape)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:], norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "\n",
        "    if model.training and idx % print_every == 0:\n",
        "      print(\"iteration: {}, time: {}, loss: {:0.4f}\".format(idx, util.time_since(start_time), loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  criterion = nn.NLLLoss(reduction = \"sum\", ignore_index = PAD_INDEX)   # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when computing the loss.\n",
        "  optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "  dev_ppls = []   # keep track of dev perplexity for each epoch\n",
        "  start = time.time()\n",
        "\n",
        "  for idx in range(num_epochs):\n",
        "    print(\"epoch\", idx)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader = train_data_loader, model = model,\n",
        "                          loss_compute = util.SimpleLossCompute(model, criterion, optim),\n",
        "                          print_every = print_every, start_time = start)\n",
        "         \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader = val_data_loader, model = model,\n",
        "                          loss_compute = util.SimpleLossCompute(model, criterion, None),\n",
        "                          print_every = print_every, start_time = start)\n",
        "      \n",
        "      print(\"validation perplexity: {:0.4f}\".format(dev_ppl))\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls\n",
        "\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.00\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "  encoder = Encoder(embed_size, hidden_size, dropout = dropout),\n",
        "  decoder = Decoder(embed_size, hidden_size, dropout = dropout),\n",
        "  src_embed = nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed = nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator = Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "\n",
        "pure_dev_ppls = train(pure_seq2seq, num_epochs, learning_rate, print_every = 100) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "iteration: 0, time: 0m 2s, loss: 325.0088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-9854e72ff605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mpure_dev_ppls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpure_seq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-9854e72ff605>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, learning_rate, print_every)\u001b[0m\n\u001b[1;32m     57\u001b[0m     train_ppl = run_epoch(data_loader = train_data_loader, model = model,\n\u001b[1;32m     58\u001b[0m                           \u001b[0mloss_compute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleLossCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                           print_every = print_every, start_time = start)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-9854e72ff605>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, model, loss_compute, print_every, start_time)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc_ids_BxT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_ids_BxL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# We define some notations here to help you understand the loaded tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# shapes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRsfDg0wCa7U",
        "colab_type": "text"
      },
      "source": [
        "#### **Evaluation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUxzn7mXkdh",
        "colab_type": "text"
      },
      "source": [
        "##### **Perplexity**\n",
        "\n",
        "Plot the perplexity graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTApnlT53YvT",
        "colab_type": "code",
        "outputId": "019f23dd-50f0-4ccb-c6c9-89c569072249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "util.plot_perplexity(pure_dev_ppls)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guoYmchOXrRp",
        "colab_type": "text"
      },
      "source": [
        "##### **Decoding**\n",
        "\n",
        "Decode the model output. For simplicity, we use greedy search here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HTqYwy6-yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n",
        "      prob = model.generator(outputs[:, -1])\n",
        "\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.data.item()\n",
        "    output.append(next_word)\n",
        "    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n",
        "\n",
        "  output = np.array(output)\n",
        "\n",
        "  # Cut off everything starting from </s>.\n",
        "  first_eos = np.where(output == EOS_INDEX)[0]\n",
        "  if len(first_eos) > 0:\n",
        "    output = output[:first_eos[0]]\n",
        "  return output\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab):\n",
        "  return [vocab[i] for i in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5go3VxJZKh",
        "colab_type": "text"
      },
      "source": [
        "Print the top 3 examples from the data loader by applying the greedy decoder. Here we use the validation dataset to print examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3m4optFrb3",
        "colab_type": "code",
        "outputId": "82e696e1-fdc9-4048-b2dc-470f0273c1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "NUM_EXAMPLES = 3\n",
        "SRC_VOCAB_SET = src_vocab_set\n",
        "TRG_VOCAB_SET = trg_vocab_set\n",
        "\n",
        "def print_examples(model, data_loader):\n",
        "  \"\"\"Prints NUM_EXAMPLES. Assumes a batch size of 1.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for idx, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device), max_len = MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    print(\"example \", idx + 1)\n",
        "    print(\"src: \", \" \".join(lookup_words(src_ids, vocab = SRC_VOCAB_SET)))\n",
        "    print(\"trg: \", \" \".join(lookup_words(trg_ids, vocab = TRG_VOCAB_SET)))\n",
        "    print(\"pred: \", \" \".join(lookup_words(result, vocab = TRG_VOCAB_SET)))\n",
        "    print()\n",
        "\n",
        "    if idx == NUM_EXAMPLES - 1:\n",
        "      break\n",
        "\n",
        "\n",
        "example_set = util.MTDataset(val_src_sentences_list, src_vocab_set, val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size = 1, num_workers = 1, shuffle = False)\n",
        "\n",
        "print_examples(pure_seq2seq, example_data_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example  1\n",
            "src:  C h <unk> n <unk> <unk> t ô i <unk> c h o <unk> n ó <unk> n ổ <unk> v à <unk> x e m <unk> x <unk> t <unk> t ừ n <unk> <unk> m ả n h <unk> n h <unk> <unk> .\n",
            "trg:  W <unk> <unk> b l o <unk> <unk> i <unk> <unk> <unk> <unk> <unk> a n d <unk> l o o <unk> <unk> a <unk> <unk> <unk> <unk> <unk> <unk> <unk> i <unk> <unk> <unk> s <unk> .\n",
            "pred:  W <unk> <unk> <unk> a <unk> o s ; <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "example  2\n",
            "src:  D <unk> <unk> v <unk> y <unk> , <unk> c h <unk> n <unk> <unk> t ô i <unk> v <unk> n <unk> x e m <unk> x <unk> t <unk> t ừ n <unk> <unk> m ả n h <unk> n h <unk> <unk> .\n",
            "trg:  B <unk> <unk> <unk> s <unk> i l l <unk> , <unk> <unk> <unk> <unk> l o o <unk> <unk> a <unk> <unk> <unk> <unk> <unk> <unk> <unk> i <unk> <unk> <unk> s <unk> .\n",
            "pred:  I <unk> <unk> <unk> a <unk> o s ; s <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "example  3\n",
            "src:  V à <unk> t ừ <unk> <unk> ư <unk> i <unk> <unk> <unk> t <unk> .\n",
            "trg:  A n d <unk> <unk> <unk> i s <unk> i s <unk> <unk> <unk> <unk> <unk> <unk> o <unk> <unk> <unk> <unk> <unk> <unk> o <unk> <unk> b <unk> l o <unk> <unk> .\n",
            "pred:  A n d <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pTV5PqJtX4",
        "colab_type": "text"
      },
      "source": [
        "##### **BLEU score**\n",
        "\n",
        "Compute the [BLEU](https://en.wikipedia.org/wiki/BLEU) score, a standard measure to evaluate the translation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XGQYwHRPyne",
        "colab_type": "code",
        "outputId": "daa94c36-4c77-4b15-9a8a-45fdcf06285f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def compute_bleu(model, data_loader):\n",
        "  bleu_score = []\n",
        "\n",
        "  model.eval()\n",
        "  for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device), max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    pred = \" \".join(lookup_words(result, vocab = TRG_VOCAB_SET))\n",
        "    targ = \" \".join(lookup_words(trg_ids, vocab = TRG_VOCAB_SET))\n",
        "\n",
        "    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n",
        "\n",
        "  return bleu_score\n",
        "\n",
        "\n",
        "test_set = MTDataset(test_src_sentences_list, SRC_VOCAB_SET, test_trg_sentences_list, TRG_VOCAB_SET, sampling = 1.)\n",
        "test_data_loader = data.DataLoader(test_set, batch_size = 1, num_workers = 8, shuffle = False)\n",
        "\n",
        "print(\"\\nBLEU score: {:0.4f}\".format(np.mean(compute_bleu(pure_seq2seq, test_data_loader))))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 21.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BLEU score: 25.0346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}